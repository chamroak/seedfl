{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41697590-46ff-48dd-b5c4-9ed872412dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MeZO'...\n",
      "remote: Enumerating objects: 173, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 173 (delta 45), reused 30 (delta 30), pack-reused 98 (from 1)\u001b[K\n",
      "Receiving objects: 100% (173/173), 432.68 KiB | 8.16 MiB/s, done.\n",
      "Resolving deltas: 100% (88/88), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/princeton-nlp/MeZO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e9fea-5199-4a71-bd6c-621d1d5d5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest compatible PyTorch version with CUDA 11.8\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.28.1\n",
    "!sudo apt-get install jq\n",
    "!pip install loralib\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4627227-0e62-47a6-aeb1-c77c6e1d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import time\n",
    "import csv\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetPowerUsage, nvmlShutdown\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "from pynvml import *\n",
    "from threading import Thread\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d4732-00e5-4d6e-ab94-77a1c524f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd MeZO/medium_models/data\n",
    "!bash download_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed0ebeb3-f051-47a3-bd18-60469e7eca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 512\n",
      "Seed = 1\n",
      "| Task = SST-2\n"
     ]
    }
   ],
   "source": [
    "# %cd medium_models/\n",
    "\n",
    "!python tools/generate_k_shot_data.py  --mode k-shot-1k-test --k 512 --task SST-2 --seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dac8175-1832-4987-b53d-13a7a0c56f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/k-shot-1k-test/SST-2/512-1/train_part1.tsv\n",
      "Saved: data/k-shot-1k-test/SST-2/512-1/train_part2.tsv\n",
      "Saved: data/k-shot-1k-test/SST-2/512-1/train_part3.tsv\n",
      "Saved: data/k-shot-1k-test/SST-2/512-1/train_part4.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def split_train_tsv(file_path, num_parts=4, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header = lines[0]\n",
    "    data_lines = lines[1:]\n",
    "\n",
    "    # Group by label\n",
    "    label_dict = {}\n",
    "    for line in data_lines:\n",
    "        label = line.strip().split('\\t')[-1]\n",
    "        if label not in label_dict:\n",
    "            label_dict[label] = []\n",
    "        label_dict[label].append(line)\n",
    "\n",
    "    # Ensure we have only two labels\n",
    "    assert len(label_dict) == 2, f\"Expected binary classification, found labels: {list(label_dict.keys())}\"\n",
    "\n",
    "    # Shuffle each label group\n",
    "    for label in label_dict:\n",
    "        random.shuffle(label_dict[label])\n",
    "\n",
    "    # Calculate min samples per label\n",
    "    min_len = min(len(label_dict[label]) for label in label_dict)\n",
    "    part_len = min_len // num_parts\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        part_lines = []\n",
    "        for label in sorted(label_dict.keys()):\n",
    "            start_idx = i * part_len\n",
    "            end_idx = (i + 1) * part_len\n",
    "            part_lines.extend(label_dict[label][start_idx:end_idx])\n",
    "\n",
    "        # Shuffle mixed part lines before saving\n",
    "        random.shuffle(part_lines)\n",
    "\n",
    "        part_path = file_path.replace(\"train.tsv\", f\"train_part{i+1}.tsv\")\n",
    "        with open(part_path, \"w\") as out:\n",
    "            out.write(header)\n",
    "            for line in part_lines:\n",
    "                out.write(line)\n",
    "\n",
    "        print(f\"Saved: {part_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# Replace this path with your actual generated file\n",
    "split_train_tsv(\"data/k-shot-1k-test/SST-2/512-1/train.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ed507ec-99d9-496f-bdff-cdbb30659383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm data/k-shot-1k-test/SST-2/128-42/train.tsv\n",
    "!mv data/k-shot-1k-test/SST-2/128-1/train.tsv data/k-shot-1k-test/SST-2/128-1/train_part1.tsv\n",
    "!mv data/k-shot-1k-test/SST-2/128-1/train_part2.tsv data/k-shot-1k-test/SST-2/128-1/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001e94b-6e36-440e-8523-de9fa8334125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Specify the directory where the cached files are located\n",
    "directory = \"data/k-shot-1k-test/SST-2/512-1/\"\n",
    "\n",
    "# Use glob to find all files starting with \"cached\"\n",
    "cached_files = glob.glob(os.path.join(directory, \"cached*\"))\n",
    "\n",
    "# Delete them\n",
    "for file_path in cached_files:\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path}: {e}\")\n",
    "        \n",
    "!WANDB_MODE=offline TASK=SST-2 K=128 SEED=1 BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL=roberta-large bash mezo.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c504cf-f73c-4920-8d34-a90fd3d896de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import gc\n",
    "    import glob\n",
    "    num_rounds = 6\n",
    "    clients = 1\n",
    "\n",
    "    K=16\n",
    "    SEED=42\n",
    "    STEP=10\n",
    "    EVAL_STEP=10\n",
    "\n",
    "    parent_folder = \"output_model\"\n",
    "    global_model_output_dir = \"saved_model\"\n",
    "    \"\"\"\n",
    "    #Load RoBERTa-large model\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2)\n",
    "    model.save_pretrained(global_model_output_dir)\n",
    "    tokenizer.save_pretrained(global_model_output_dir)\n",
    "    \"\"\"\n",
    "    # for client in range (1, clients+1):\n",
    "    #     command = (f\"python tools/generate_k_shot_data.py  --mode k-shot-1k-test --k 128 --task SST-2 --seed {client}\")\n",
    "    #     get_ipython().system(command)\n",
    "    \n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "\n",
    "      print(f\"Starting Round {round_num}\")\n",
    "      averaged_state_dict = {}\n",
    "      \"\"\"\n",
    "      model = RobertaForSequenceClassification.from_pretrained(global_model_output_dir)\n",
    "      tokenizer = RobertaTokenizer.from_pretrained(global_model_output_dir)\n",
    "      print(f\"Parameters before pruning: {count_parameters(model)}\")\n",
    "      model = prune_model(model, 0.5)\n",
    "      print(f\"Active parameters after pruning: {count_active_parameters(model)}\")\n",
    "      model.save_pretrained(global_model_output_dir)\n",
    "      tokenizer.save_pretrained(global_model_output_dir)\n",
    "      \"\"\"\n",
    "\n",
    "      for client_id in range (1, clients + 1):\n",
    "        # base_adr = f'round{round_num}/client{client_id}'\n",
    "        # os.makedirs(base_adr, exist_ok=True)  # Creates the directories if they don't exist\n",
    "\n",
    "        # input_file_path = f'{base_adr}/logs_K{K}_SEED{SEED}_STEP{STEP}_EVALSTEP{EVAL_STEP}.txt'\n",
    "        # output_pdf_path = f'{base_adr}/plos_K{K}_SEED{SEED}_STEP{STEP}_EVALSTEP{EVAL_STEP}.pdf'\n",
    "        # energy_log_mezo = f'{base_adr}/energy_log_mezo_{K}_SEED{SEED}_STEP{STEP}_EVALSTEP{EVAL_STEP}.csv'\n",
    "\n",
    "        print(f\"------------------------------------Executing command for Client {client_id} in Round {round_num}:\")\n",
    "        # Specify the directory where the cached files are located\n",
    "        directory = \"data/k-shot-1k-test/SST-2/512-1/\"\n",
    "        \n",
    "        # Use glob to find all files starting with \"cached\"\n",
    "        cached_files = glob.glob(os.path.join(directory, \"cached*\"))\n",
    "        \n",
    "        # Delete them\n",
    "        for file_path in cached_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "        command = (f\"mv data/k-shot-1k-test/SST-2/512-1/train_part{client_id}.tsv data/k-shot-1k-test/SST-2/512-1/train.tsv\")\n",
    "        get_ipython().system(command)\n",
    "\n",
    "        if round_num == 1:\n",
    "          print(\"we are here\")\n",
    "          command = (f\"WANDB_MODE=offline TASK=SST-2 K=512 SEED=1 BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL=roberta-large bash mezo.sh\")\n",
    "          get_ipython().system(command)\n",
    "          # measure_function_energy(mezo, args=(\"roberta-large\", input_file_path, client_id, K, SEED), interval=1, output_file=energy_log_mezo, banner=\"for mezo\")\n",
    "        else:\n",
    "          command = (f\"WANDB_MODE=offline TASK=SST-2 K=512 SEED=1 BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL={global_model_output_dir} bash mezo.sh\")\n",
    "          get_ipython().system(command)\n",
    "        command = (f\"mv data/k-shot-1k-test/SST-2/512-1/train.tsv data/k-shot-1k-test/SST-2/512-1/train_part{client_id}.tsv\")\n",
    "        get_ipython().system(command)\n",
    "          # measure_function_energy(mezo, args=(global_model_output_dir, input_file_path, client_id, K, SEED), interval=1, output_file=energy_log_mezo, banner=\"for mezo\")\n",
    "\n",
    "        model = RobertaForSequenceClassification.from_pretrained(f\"{parent_folder}\")\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(f\"{parent_folder}\")\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        print(\"create average dic\")\n",
    "        if not averaged_state_dict:\n",
    "          for key in state_dict.keys():\n",
    "                # print(\"you should see only one time\")\n",
    "                tensor = state_dict[key]\n",
    "                if tensor.is_floating_point():\n",
    "                    averaged_state_dict[key] = torch.zeros_like(tensor)\n",
    "                else:\n",
    "                    averaged_state_dict[key] = tensor.clone()\n",
    "\n",
    "        print(\"summation\")\n",
    "        for key in state_dict.keys():\n",
    "              try:\n",
    "                  tensor = state_dict[key]\n",
    "                  #print(f\"Key: {key}, Tensor Type: {tensor.dtype}, Tensor Shape: {tensor.shape}\")\n",
    "                  if tensor.is_floating_point():\n",
    "                      averaged_state_dict[key] += tensor\n",
    "                  else:\n",
    "                      averaged_state_dict[key] = tensor.clone()\n",
    "              except Exception as e:\n",
    "                  print(f\"Error processing key {key}: {str(e)}\")\n",
    "                  raise e\n",
    "        command = (f\"rm -rf output_model\")\n",
    "        get_ipython().system(command)\n",
    "\n",
    "      print (\"go for averaging\")\n",
    "      # Average the parameters\n",
    "      averaged_state_dict[key] /= clients\n",
    "      global_model = RobertaForSequenceClassification.from_pretrained('roberta-large')\n",
    "      global_model.load_state_dict(averaged_state_dict)\n",
    "      print(\"update\")\n",
    "      global_model.save_pretrained(global_model_output_dir)\n",
    "      tokenizer.save_pretrained(global_model_output_dir)\n",
    "      print(\"done saving in this round\")\n",
    "\n",
    "\n",
    "    print(\"All rounds completed.\")\n",
    "    print(\"Generate Plots\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef190cc6-fd52-4fc0-8d3a-463297def1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javad/MeZO/medium_models\n"
     ]
    }
   ],
   "source": [
    "%cd MeZO/medium_models/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
