{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41697590-46ff-48dd-b5c4-9ed872412dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MeZO'...\n",
      "remote: Enumerating objects: 173, done.\u001b[K\n",
      "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 173 (delta 45), reused 30 (delta 30), pack-reused 98 (from 1)\u001b[K\n",
      "Receiving objects: 100% (173/173), 432.68 KiB | 8.16 MiB/s, done.\n",
      "Resolving deltas: 100% (88/88), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/princeton-nlp/MeZO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3e9fea-5199-4a71-bd6c-621d1d5d5405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.0+cu121)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting transformers==4.28.1\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.1)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.28.1)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers==4.28.1) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.28.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.28.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.28.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.28.1) (2024.7.4)\n",
      "Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.30.2 regex-2024.11.6 tokenizers-0.13.3 transformers-4.28.1\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "E: Unable to locate package jq\n",
      "Collecting loralib\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: loralib\n",
      "Successfully installed loralib-0.1.2\n",
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=4d7724e306acd2248551af91ca91d487cdd36db04f7871eddbb4d1ad7ec869cd\n",
      "  Stored in directory: /home/javad/.cache/pip/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, multiprocess, datasets\n",
      "Successfully installed datasets-3.5.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Install the latest compatible PyTorch version with CUDA 11.8\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.28.1\n",
    "!sudo apt-get install jq\n",
    "!pip install loralib\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944e63a7-8f4c-4277-8d97-4640831f8103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2788 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1243 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4000 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4154 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1542 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3099 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
      "Fetched 37.4 MB in 3s (11.2 MB/s)                         \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libjq1 libonig5\n",
      "The following NEW packages will be installed:\n",
      "  jq libjq1 libonig5\n",
      "0 upgraded, 3 newly installed, 0 to remove and 78 not upgraded.\n",
      "Need to get 357 kB of archives.\n",
      "After this operation, 1087 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libonig5 amd64 6.9.7.1-2build1 [172 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjq1 amd64 1.6-2.1ubuntu3 [133 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 jq amd64 1.6-2.1ubuntu3 [52.5 kB]\n",
      "Fetched 357 kB in 1s (585 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libonig5:amd64.\n",
      "(Reading database ... 49713 files and directories currently installed.)\n",
      "Preparing to unpack .../libonig5_6.9.7.1-2build1_amd64.deb ...\n",
      "Unpacking libonig5:amd64 (6.9.7.1-2build1) ...\n",
      "Selecting previously unselected package libjq1:amd64.\n",
      "Preparing to unpack .../libjq1_1.6-2.1ubuntu3_amd64.deb ...\n",
      "Unpacking libjq1:amd64 (1.6-2.1ubuntu3) ...\n",
      "Selecting previously unselected package jq.\n",
      "Preparing to unpack .../jq_1.6-2.1ubuntu3_amd64.deb ...\n",
      "Unpacking jq (1.6-2.1ubuntu3) ...\n",
      "Setting up libonig5:amd64 (6.9.7.1-2build1) ...\n",
      "Setting up libjq1:amd64 (1.6-2.1ubuntu3) ...\n",
      "Setting up jq (1.6-2.1ubuntu3) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install jq -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ccf936-e3d3-4779-8c6e-4d61c96c9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/jq\n"
     ]
    }
   ],
   "source": [
    "!which jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4627227-0e62-47a6-aeb1-c77c6e1d7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import time\n",
    "import csv\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetPowerUsage, nvmlShutdown\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "from pynvml import *\n",
    "from threading import Thread\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936d4732-00e5-4d6e-ab94-77a1c524f8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javad/seedfl/medium_models/data\n",
      "--2025-04-10 15:12:44--  https://nlp.cs.princeton.edu/projects/lm-bff/datasets.tar\n",
      "Resolving nlp.cs.princeton.edu (nlp.cs.princeton.edu)... 128.112.136.67\n",
      "Connecting to nlp.cs.princeton.edu (nlp.cs.princeton.edu)|128.112.136.67|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1901486080 (1.8G) [application/x-tar]\n",
      "Saving to: ‘datasets.tar’\n",
      "\n",
      "datasets.tar        100%[===================>]   1.77G  35.5MB/s    in 52s     \n",
      "\n",
      "2025-04-10 15:13:37 (34.7 MB/s) - ‘datasets.tar’ saved [1901486080/1901486080]\n",
      "\n",
      "original/\n",
      "original/CoLA/\n",
      "original/CoLA/test.tsv\n",
      "original/CoLA/dev.tsv\n",
      "original/CoLA/train.tsv\n",
      "original/GLUE-SST-2/\n",
      "original/GLUE-SST-2/test.tsv\n",
      "original/GLUE-SST-2/dev.tsv\n",
      "original/GLUE-SST-2/train.tsv\n",
      "original/mr/\n",
      "original/mr/process.py\n",
      "original/mr/mr.all\n",
      "original/mr/train.csv\n",
      "original/mr/test.csv\n",
      "original/trec/\n",
      "original/trec/TREC.test.all\n",
      "original/trec/process.py\n",
      "original/trec/train.csv\n",
      "original/trec/test.csv\n",
      "original/trec/TREC.train.all\n",
      "original/WNLI/\n",
      "original/WNLI/test.tsv\n",
      "original/WNLI/dev.tsv\n",
      "original/WNLI/train.tsv\n",
      "original/SNLI/\n",
      "original/SNLI/test.tsv\n",
      "original/SNLI/original/\n",
      "original/SNLI/original/snli_1.0_dev.jsonl\n",
      "original/SNLI/original/snli_1.0_test.jsonl\n",
      "original/SNLI/original/snli_1.0_train.txt\n",
      "original/SNLI/original/snli_1.0_test.txt\n",
      "original/SNLI/original/snli_1.0_train.jsonl\n",
      "original/SNLI/original/snli_1.0_dev.txt\n",
      "original/SNLI/dev.tsv\n",
      "original/SNLI/README.txt\n",
      "original/SNLI/train.tsv\n",
      "original/mpqa/\n",
      "original/mpqa/process.py\n",
      "original/mpqa/train.csv\n",
      "original/mpqa/test.csv\n",
      "original/mpqa/mpqa.all\n",
      "original/QQP/\n",
      "original/QQP/test.tsv\n",
      "original/QQP/dev.tsv\n",
      "original/QQP/train.tsv\n",
      "original/STS-B/\n",
      "original/STS-B/test.tsv\n",
      "original/STS-B/readme.txt\n",
      "original/STS-B/original/\n",
      "original/STS-B/original/sts-test.tsv\n",
      "original/STS-B/original/sts-dev.tsv\n",
      "original/STS-B/original/sts-train.tsv\n",
      "original/STS-B/LICENSE.txt\n",
      "original/STS-B/dev.tsv\n",
      "original/STS-B/train.tsv\n",
      "original/RTE/\n",
      "original/RTE/test.tsv\n",
      "original/RTE/dev.tsv\n",
      "original/RTE/train.tsv\n",
      "original/subj/\n",
      "original/subj/process.py\n",
      "original/subj/subj.all\n",
      "original/subj/train.csv\n",
      "original/subj/test.csv\n",
      "original/QNLI/\n",
      "original/QNLI/test.tsv\n",
      "original/QNLI/dev.tsv\n",
      "original/QNLI/train.tsv\n",
      "original/MNLI/\n",
      "original/MNLI/test_matched.tsv\n",
      "original/MNLI/test_mismatched.tsv\n",
      "original/MNLI/dev_matched.tsv\n",
      "original/MNLI/README.txt\n",
      "original/MNLI/train.tsv\n",
      "original/MNLI/dev_mismatched.tsv\n",
      "original/cr/\n",
      "original/cr/custrev.all\n",
      "original/cr/process.py\n",
      "original/cr/train.csv\n",
      "original/cr/test.csv\n",
      "original/MRPC/\n",
      "original/MRPC/test.tsv\n",
      "original/MRPC/msr_paraphrase_test.txt\n",
      "original/MRPC/msr_paraphrase_train.txt\n",
      "original/MRPC/dev.tsv\n",
      "original/MRPC/dev_ids.tsv\n",
      "original/MRPC/train.tsv\n",
      "original/SST-2/\n",
      "original/SST-2/test.tsv\n",
      "original/SST-2/transfer.py\n",
      "original/SST-2/dev.tsv\n",
      "original/SST-2/train.tsv\n",
      "original/sst-5/\n",
      "original/sst-5/process.py\n",
      "original/sst-5/stsa.fine.test\n",
      "original/sst-5/stsa.fine.dev\n",
      "original/sst-5/stsa.fine.train\n",
      "original/sst-5/train.csv\n",
      "original/sst-5/test.csv\n",
      "*** Use GLUE-SST-2 as default SST-2 ***\n",
      "*** Done ***\n"
     ]
    }
   ],
   "source": [
    "%cd medium_models/data\n",
    "!bash download_dataset.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb083e5c-14e4-46fa-8506-8b867095befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  finetune.sh\tlog.lock  result  run_fewshot.sh  src\n",
      "data\t   log\t\tmezo.sh   run.py  saved_model\t  tools\n"
     ]
    }
   ],
   "source": [
    "!ls seedfl/medium_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0ebeb3-f051-47a3-bd18-60469e7eca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javad/seedfl/medium_models\n",
      "K = 512\n",
      "Seed = 1\n",
      "| Task = SST-2\n",
      "K = 512\n",
      "Seed = 2\n",
      "| Task = SST-2\n",
      "K = 512\n",
      "Seed = 3\n",
      "| Task = SST-2\n",
      "K = 512\n",
      "Seed = 4\n",
      "| Task = SST-2\n"
     ]
    }
   ],
   "source": [
    "%cd seedfl/medium_models\n",
    "\n",
    "!python tools/generate_k_shot_data.py  --mode k-shot-1k-test --k 512 --task SST-2 --seed 1\n",
    "!python tools/generate_k_shot_data.py  --mode k-shot-1k-test --k 512 --task SST-2 --seed 2\n",
    "!python tools/generate_k_shot_data.py  --mode k-shot-1k-test --k 512 --task SST-2 --seed 3\n",
    "!python tools/generate_k_shot_data.py  --mode k-shot-1k-test --k 512 --task SST-2 --seed 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dac8175-1832-4987-b53d-13a7a0c56f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/k-shot-1k-test/SST-2/512-1/train_part1.tsv\n",
      "Saved: data/k-shot-1k-test/SST-2/512-2/train_part1.tsv\n",
      "Saved: data/k-shot-1k-test/SST-2/512-3/train_part1.tsv\n",
      "Saved: data/k-shot-1k-test/SST-2/512-4/train_part1.tsv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def split_train_tsv(file_path, num_parts=1, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header = lines[0]\n",
    "    data_lines = lines[1:]\n",
    "\n",
    "    # Group by label\n",
    "    label_dict = {}\n",
    "    for line in data_lines:\n",
    "        label = line.strip().split('\\t')[-1]\n",
    "        if label not in label_dict:\n",
    "            label_dict[label] = []\n",
    "        label_dict[label].append(line)\n",
    "\n",
    "    # Ensure we have only two labels\n",
    "    assert len(label_dict) == 2, f\"Expected binary classification, found labels: {list(label_dict.keys())}\"\n",
    "\n",
    "    # Shuffle each label group\n",
    "    for label in label_dict:\n",
    "        random.shuffle(label_dict[label])\n",
    "\n",
    "    # Calculate min samples per label\n",
    "    min_len = min(len(label_dict[label]) for label in label_dict)\n",
    "    part_len = min_len // num_parts\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        part_lines = []\n",
    "        for label in sorted(label_dict.keys()):\n",
    "            start_idx = i * part_len\n",
    "            end_idx = (i + 1) * part_len\n",
    "            part_lines.extend(label_dict[label][start_idx:end_idx])\n",
    "\n",
    "        # Shuffle mixed part lines before saving\n",
    "        random.shuffle(part_lines)\n",
    "\n",
    "        part_path = file_path.replace(\"train.tsv\", f\"train_part{i+1}.tsv\")\n",
    "        with open(part_path, \"w\") as out:\n",
    "            out.write(header)\n",
    "            for line in part_lines:\n",
    "                out.write(line)\n",
    "\n",
    "        print(f\"Saved: {part_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# Replace this path with your actual generated file\n",
    "for i in range(1,5):\n",
    "    split_train_tsv(f\"data/k-shot-1k-test/SST-2/512-{i}/train.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ed507ec-99d9-496f-bdff-cdbb30659383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm data/k-shot-1k-test/SST-2/128-42/train.tsv\n",
    "!mv data/k-shot-1k-test/SST-2/128-1/train.tsv data/k-shot-1k-test/SST-2/128-1/train_part1.tsv\n",
    "!mv data/k-shot-1k-test/SST-2/128-1/train_part2.tsv data/k-shot-1k-test/SST-2/128-1/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001e94b-6e36-440e-8523-de9fa8334125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Specify the directory where the cached files are located\n",
    "directory = \"data/k-shot-1k-test/SST-2/512-1/\"\n",
    "\n",
    "# Use glob to find all files starting with \"cached\"\n",
    "cached_files = glob.glob(os.path.join(directory, \"cached*\"))\n",
    "\n",
    "# Delete them\n",
    "for file_path in cached_files:\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {file_path}: {e}\")\n",
    "        \n",
    "!WANDB_MODE=offline TASK=SST-2 K=128 SEED=1 BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL=roberta-large bash mezo.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c504cf-f73c-4920-8d34-a90fd3d896de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Round 1\n",
      "------------------------------------Executing command for Client 1 in Round 1:\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 1\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-roberta-large-mezo-ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2025 15:24:14 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:24:14 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1/runs/Apr10_15-24-14_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=1,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:24:14 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:24:16 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-1/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.007 s]\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-1/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.020 s]\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 354, 1085, 53, 33750, 29098, 43848, 5739, 31, 386, 7, 2073, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=0, mask_pos=[15], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   text: <s>is nothing but boilerplate clichés from start to finish  It was<mask>.</s>\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-1/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.013 s]\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:24:21 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:24:23 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:24:23 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:24:23 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:24:23 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:24:23 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:24:23 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:24:23 - INFO - src.trainer -     Total optimization steps = 10\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:24:25 - INFO - src.trainer -   {'loss': 0.4579552114009857, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 6/256 [00:00<00:05, 49.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:05<00:00, 44.59it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:24:39 - INFO - src.trainer -   {'eval_loss': 0.4969324767589569, 'eval_acc': 0.75390625}\n",
      "04/10/2025 15:24:39 - INFO - src.trainer -   Best dev result: 0.75390625\n",
      "04/10/2025 15:24:41 - INFO - src.trainer -   {'loss': 0.5381416082382202, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 17}\n",
      "04/10/2025 15:24:41 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:24:44 - INFO - __main__ -   *** Validate ***\n",
      "508it [00:17, 45.84it/s]04/10/2025 15:24:50 - INFO - src.trainer -   {'eval_loss': 0.4969324767589569, 'eval_acc': 0.75390625}\n",
      "04/10/2025 15:24:50 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:24:50 - INFO - __main__ -     eval_loss = 0.4969324767589569\n",
      "04/10/2025 15:24:50 - INFO - __main__ -     eval_acc = 0.75390625\n",
      "04/10/2025 15:24:50 - INFO - root -   *** Test ***\n",
      "729it [00:23, 33.73it/s]04/10/2025 15:24:56 - INFO - src.trainer -   {'eval_loss': 0.3953275978565216, 'eval_acc': 0.8245412844036697}\n",
      "04/10/2025 15:24:56 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:24:56 - INFO - __main__ -     eval_loss = 0.3953275978565216\n",
      "04/10/2025 15:24:56 - INFO - __main__ -     eval_acc = 0.8245412844036697\n",
      "04/10/2025 15:24:56 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:24:56 - INFO - __main__ -   result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1\n",
      "730it [00:23, 31.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeZO stdout:\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 1\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-roberta-large-mezo-ft\n",
      "04/10/2025 15:25:02 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:25:02 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1/runs/Apr10_15-25-02_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=1,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:25:02 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:25:03 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'roberta.embeddings.position_ids', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "/home/javad/seedfl/medium_models/src/dataset.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.support_examples, self.query_examples = torch.load(cached_features_file)\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-1/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.003 s]\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-1/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.013 s]\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 354, 1085, 53, 33750, 29098, 43848, 5739, 31, 386, 7, 2073, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=0, mask_pos=[15], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   text: <s>is nothing but boilerplate clichés from start to finish  It was<mask>.</s>\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:25:08 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-1/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.006 s]\n",
      "04/10/2025 15:25:09 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:25:09 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:25:09 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:25:09 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:25:11 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:25:11 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:25:11 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:25:11 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:25:11 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:25:11 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:25:11 - INFO - src.trainer -     Total optimization steps = 10\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:25:12 - INFO - src.trainer -   {'loss': 0.4579552114009857, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\n",
      "  0%|          | 0/256 [00:00<?, ?it/s]\n",
      "  2%|▏         | 6/256 [00:00<00:05, 49.79it/s]\n",
      "  4%|▍         | 11/256 [00:00<00:05, 47.39it/s]\n",
      "  6%|▋         | 16/256 [00:00<00:05, 46.07it/s]\n",
      "  8%|▊         | 21/256 [00:00<00:05, 42.58it/s]\n",
      " 10%|█         | 26/256 [00:00<00:05, 42.44it/s]\n",
      " 12%|█▏        | 31/256 [00:00<00:05, 41.70it/s]\n",
      " 14%|█▍        | 36/256 [00:00<00:05, 42.51it/s]\n",
      " 16%|█▌        | 41/256 [00:00<00:04, 43.76it/s]\n",
      " 18%|█▊        | 46/256 [00:01<00:04, 43.89it/s]\n",
      " 20%|█▉        | 51/256 [00:01<00:04, 44.84it/s]\n",
      " 22%|██▏       | 56/256 [00:01<00:04, 43.66it/s]\n",
      " 24%|██▍       | 61/256 [00:01<00:04, 43.72it/s]\n",
      " 26%|██▌       | 66/256 [00:01<00:04, 41.22it/s]\n",
      " 28%|██▊       | 71/256 [00:01<00:04, 41.58it/s]\n",
      " 30%|██▉       | 76/256 [00:01<00:04, 42.04it/s]\n",
      " 32%|███▏      | 81/256 [00:01<00:04, 41.70it/s]\n",
      " 34%|███▎      | 86/256 [00:01<00:04, 42.33it/s]\n",
      " 36%|███▌      | 92/256 [00:02<00:03, 44.87it/s]\n",
      " 38%|███▊      | 97/256 [00:02<00:03, 45.67it/s]\n",
      " 40%|███▉      | 102/256 [00:02<00:03, 44.04it/s]\n",
      " 42%|████▏     | 107/256 [00:02<00:03, 44.99it/s]\n",
      " 44%|████▍     | 112/256 [00:02<00:03, 43.78it/s]\n",
      " 46%|████▌     | 117/256 [00:02<00:03, 43.10it/s]\n",
      " 48%|████▊     | 122/256 [00:02<00:03, 43.38it/s]\n",
      " 50%|████▉     | 127/256 [00:02<00:03, 41.94it/s]\n",
      " 52%|█████▏    | 132/256 [00:03<00:02, 42.71it/s]\n",
      " 54%|█████▎    | 137/256 [00:03<00:02, 43.64it/s]\n",
      " 55%|█████▌    | 142/256 [00:03<00:02, 43.60it/s]\n",
      " 57%|█████▋    | 147/256 [00:03<00:02, 43.62it/s]\n",
      " 59%|█████▉    | 152/256 [00:03<00:02, 44.71it/s]\n",
      " 61%|██████▏   | 157/256 [00:03<00:02, 45.35it/s]\n",
      " 64%|██████▎   | 163/256 [00:03<00:01, 47.15it/s]\n",
      " 66%|██████▌   | 168/256 [00:03<00:01, 46.91it/s]\n",
      " 68%|██████▊   | 174/256 [00:03<00:01, 47.88it/s]\n",
      " 70%|██████▉   | 179/256 [00:04<00:01, 45.34it/s]\n",
      " 72%|███████▏  | 184/256 [00:04<00:01, 44.36it/s]\n",
      " 74%|███████▍  | 190/256 [00:04<00:01, 46.59it/s]\n",
      " 76%|███████▌  | 195/256 [00:04<00:01, 45.98it/s]\n",
      " 78%|███████▊  | 200/256 [00:04<00:01, 43.62it/s]\n",
      " 80%|████████  | 205/256 [00:04<00:01, 43.40it/s]\n",
      " 82%|████████▏ | 210/256 [00:04<00:01, 44.70it/s]\n",
      " 84%|████████▍ | 215/256 [00:04<00:00, 45.28it/s]\n",
      " 86%|████████▌ | 220/256 [00:04<00:00, 46.08it/s]\n",
      " 88%|████████▊ | 226/256 [00:05<00:00, 47.72it/s]\n",
      " 90%|█████████ | 231/256 [00:05<00:00, 46.72it/s]\n",
      " 92%|█████████▏| 236/256 [00:05<00:00, 45.04it/s]\n",
      " 94%|█████████▍| 241/256 [00:05<00:00, 44.64it/s]\n",
      " 96%|█████████▋| 247/256 [00:05<00:00, 46.60it/s]\n",
      " 98%|█████████▊| 252/256 [00:05<00:00, 45.17it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:25:26 - INFO - src.trainer -   {'eval_loss': 0.4969324767589569, 'eval_acc': 0.75390625}\n",
      "04/10/2025 15:25:26 - INFO - src.trainer -   Best dev result: 0.75390625\n",
      "04/10/2025 15:25:28 - INFO - src.trainer -   {'loss': 0.5381416082382202, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 17}\n",
      "04/10/2025 15:25:28 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:25:32 - INFO - __main__ -   *** Validate ***\n",
      "\n",
      "257it [00:11,  2.69it/s]                         \n",
      "261it [00:11,  3.48it/s]\n",
      "265it [00:12,  4.57it/s]\n",
      "270it [00:12,  6.40it/s]\n",
      "275it [00:12,  8.73it/s]\n",
      "279it [00:12, 10.97it/s]\n",
      "283it [00:12, 13.55it/s]\n",
      "288it [00:12, 17.51it/s]\n",
      "293it [00:12, 21.89it/s]\n",
      "298it [00:12, 25.73it/s]\n",
      "303it [00:12, 29.98it/s]\n",
      "308it [00:13, 33.75it/s]\n",
      "313it [00:13, 35.90it/s]\n",
      "318it [00:13, 37.46it/s]\n",
      "323it [00:13, 37.68it/s]\n",
      "328it [00:13, 38.83it/s]\n",
      "333it [00:13, 39.11it/s]\n",
      "338it [00:13, 40.56it/s]\n",
      "343it [00:13, 41.63it/s]\n",
      "349it [00:14, 44.31it/s]\n",
      "354it [00:14, 45.05it/s]\n",
      "359it [00:14, 43.78it/s]\n",
      "364it [00:14, 44.59it/s]\n",
      "369it [00:14, 43.67it/s]\n",
      "374it [00:14, 42.54it/s]\n",
      "379it [00:14, 42.45it/s]\n",
      "384it [00:14, 41.96it/s]\n",
      "389it [00:14, 42.70it/s]\n",
      "394it [00:15, 43.75it/s]\n",
      "399it [00:15, 43.83it/s]\n",
      "404it [00:15, 43.76it/s]\n",
      "409it [00:15, 44.86it/s]\n",
      "414it [00:15, 45.56it/s]\n",
      "420it [00:15, 46.64it/s]\n",
      "425it [00:15, 46.43it/s]\n",
      "430it [00:15, 47.31it/s]\n",
      "435it [00:15, 44.89it/s]\n",
      "440it [00:16, 43.95it/s]\n",
      "446it [00:16, 46.18it/s]\n",
      "451it [00:16, 45.69it/s]\n",
      "456it [00:16, 43.50it/s]\n",
      "461it [00:16, 43.33it/s]\n",
      "466it [00:16, 44.64it/s]\n",
      "471it [00:16, 45.24it/s]\n",
      "476it [00:16, 46.03it/s]\n",
      "482it [00:16, 47.58it/s]\n",
      "487it [00:17, 46.61it/s]\n",
      "492it [00:17, 44.97it/s]\n",
      "497it [00:17, 44.49it/s]\n",
      "503it [00:17, 46.68it/s]\n",
      "508it [00:17, 45.30it/s]04/10/2025 15:25:38 - INFO - src.trainer -   {'eval_loss': 0.4969324767589569, 'eval_acc': 0.75390625}\n",
      "04/10/2025 15:25:38 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:25:38 - INFO - __main__ -     eval_loss = 0.4969324767589569\n",
      "04/10/2025 15:25:38 - INFO - __main__ -     eval_acc = 0.75390625\n",
      "04/10/2025 15:25:38 - INFO - root -   *** Test ***\n",
      "\n",
      "513it [00:17, 44.92it/s]\n",
      "518it [00:17, 41.17it/s]\n",
      "523it [00:17, 39.79it/s]\n",
      "528it [00:18, 39.83it/s]\n",
      "533it [00:18, 38.56it/s]\n",
      "537it [00:18, 38.46it/s]\n",
      "541it [00:18, 38.31it/s]\n",
      "545it [00:18, 37.72it/s]\n",
      "549it [00:18, 37.88it/s]\n",
      "553it [00:18, 36.56it/s]\n",
      "558it [00:18, 37.84it/s]\n",
      "562it [00:19, 36.52it/s]\n",
      "566it [00:19, 36.36it/s]\n",
      "570it [00:19, 36.18it/s]\n",
      "574it [00:19, 35.34it/s]\n",
      "578it [00:19, 35.57it/s]\n",
      "582it [00:19, 34.97it/s]\n",
      "586it [00:19, 33.83it/s]\n",
      "590it [00:19, 35.06it/s]\n",
      "595it [00:19, 37.16it/s]\n",
      "599it [00:20, 35.34it/s]\n",
      "603it [00:20, 36.37it/s]\n",
      "607it [00:20, 35.60it/s]\n",
      "611it [00:20, 34.39it/s]\n",
      "615it [00:20, 35.67it/s]\n",
      "619it [00:20, 35.11it/s]\n",
      "623it [00:20, 34.63it/s]\n",
      "627it [00:20, 33.64it/s]\n",
      "631it [00:20, 34.20it/s]\n",
      "635it [00:21, 34.90it/s]\n",
      "640it [00:21, 36.65it/s]\n",
      "644it [00:21, 36.41it/s]\n",
      "648it [00:21, 35.71it/s]\n",
      "652it [00:21, 36.41it/s]\n",
      "656it [00:21, 35.54it/s]\n",
      "660it [00:21, 35.69it/s]\n",
      "664it [00:21, 36.03it/s]\n",
      "668it [00:22, 35.34it/s]\n",
      "673it [00:22, 36.73it/s]\n",
      "677it [00:22, 37.22it/s]\n",
      "681it [00:22, 36.98it/s]\n",
      "685it [00:22, 37.54it/s]\n",
      "689it [00:22, 36.82it/s]\n",
      "693it [00:22, 37.28it/s]\n",
      "697it [00:22, 37.37it/s]\n",
      "701it [00:22, 36.33it/s]\n",
      "705it [00:23, 34.80it/s]\n",
      "709it [00:23, 35.11it/s]\n",
      "713it [00:23, 33.84it/s]\n",
      "717it [00:23, 33.20it/s]\n",
      "721it [00:23, 34.03it/s]\n",
      "725it [00:23, 34.68it/s]\n",
      "729it [00:23, 32.99it/s]04/10/2025 15:25:44 - INFO - src.trainer -   {'eval_loss': 0.3953275978565216, 'eval_acc': 0.8245412844036697}\n",
      "04/10/2025 15:25:44 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:25:44 - INFO - __main__ -     eval_loss = 0.3953275978565216\n",
      "04/10/2025 15:25:44 - INFO - __main__ -     eval_acc = 0.8245412844036697\n",
      "04/10/2025 15:25:44 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:25:44 - INFO - __main__ -   result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1\n",
      "\n",
      "730it [00:23, 30.71it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "------------------------------------Executing command for Client 2 in Round 1:\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 2\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-roberta-large-mezo-ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2025 15:25:57 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:25:57 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2/runs/Apr10_15-25-57_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=2,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:25:57 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:25:58 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-2/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.007 s]\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:26:03 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-2/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.021 s]\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 20042, 1302, 2542, 9, 39, 308, 3035, 1825, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[12], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   text: <s>never seems aware of his own coolness  It was<mask>.</s>\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-2/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.012 s]\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:26:04 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:26:06 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:26:06 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:26:06 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:26:06 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:26:06 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:26:06 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:26:06 - INFO - src.trainer -     Total optimization steps = 10\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:26:08 - INFO - src.trainer -   {'loss': 0.5486952662467957, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 6/256 [00:00<00:05, 49.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 253/256 [00:06<00:00, 41.52it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:26:22 - INFO - src.trainer -   {'eval_loss': 0.4657633304595947, 'eval_acc': 0.7802734375}\n",
      "04/10/2025 15:26:22 - INFO - src.trainer -   Best dev result: 0.7802734375\n",
      "04/10/2025 15:26:24 - INFO - src.trainer -   {'loss': 0.5463495254516602, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 17}\n",
      "04/10/2025 15:26:24 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:26:28 - INFO - __main__ -   *** Validate ***\n",
      "512it [00:18, 41.18it/s]04/10/2025 15:26:34 - INFO - src.trainer -   {'eval_loss': 0.4657633304595947, 'eval_acc': 0.7802734375}\n",
      "04/10/2025 15:26:34 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:26:34 - INFO - __main__ -     eval_loss = 0.4657633304595947\n",
      "04/10/2025 15:26:34 - INFO - __main__ -     eval_acc = 0.7802734375\n",
      "04/10/2025 15:26:34 - INFO - root -   *** Test ***\n",
      "729it [00:24, 32.57it/s]04/10/2025 15:26:40 - INFO - src.trainer -   {'eval_loss': 0.3997124433517456, 'eval_acc': 0.8130733944954128}\n",
      "04/10/2025 15:26:40 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:26:40 - INFO - __main__ -     eval_loss = 0.3997124433517456\n",
      "04/10/2025 15:26:40 - INFO - __main__ -     eval_acc = 0.8130733944954128\n",
      "04/10/2025 15:26:40 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:26:40 - INFO - __main__ -   result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2\n",
      "730it [00:24, 29.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeZO stdout:\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 2\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-roberta-large-mezo-ft\n",
      "04/10/2025 15:26:46 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:26:46 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2/runs/Apr10_15-26-46_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=2,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:26:46 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:26:47 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "/home/javad/seedfl/medium_models/src/dataset.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.support_examples, self.query_examples = torch.load(cached_features_file)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-2/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.005 s]\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-2/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.014 s]\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 20042, 1302, 2542, 9, 39, 308, 3035, 1825, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[12], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   text: <s>never seems aware of his own coolness  It was<mask>.</s>\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-2/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.006 s]\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:26:53 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:26:55 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:26:55 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:26:55 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:26:55 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:26:55 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:26:55 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:26:55 - INFO - src.trainer -     Total optimization steps = 10\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:26:57 - INFO - src.trainer -   {'loss': 0.5486952662467957, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\n",
      "  0%|          | 0/256 [00:00<?, ?it/s]\n",
      "  2%|▏         | 6/256 [00:00<00:04, 51.23it/s]\n",
      "  5%|▍         | 12/256 [00:00<00:05, 44.90it/s]\n",
      "  7%|▋         | 17/256 [00:00<00:05, 44.68it/s]\n",
      "  9%|▊         | 22/256 [00:00<00:05, 44.92it/s]\n",
      " 11%|█         | 27/256 [00:00<00:04, 46.04it/s]\n",
      " 12%|█▎        | 32/256 [00:00<00:04, 45.04it/s]\n",
      " 14%|█▍        | 37/256 [00:00<00:04, 45.44it/s]\n",
      " 16%|█▋        | 42/256 [00:00<00:04, 46.17it/s]\n",
      " 18%|█▊        | 47/256 [00:01<00:04, 46.37it/s]\n",
      " 20%|██        | 52/256 [00:01<00:04, 46.17it/s]\n",
      " 22%|██▏       | 57/256 [00:01<00:04, 46.10it/s]\n",
      " 24%|██▍       | 62/256 [00:01<00:04, 46.33it/s]\n",
      " 26%|██▌       | 67/256 [00:01<00:04, 45.07it/s]\n",
      " 28%|██▊       | 72/256 [00:01<00:04, 43.73it/s]\n",
      " 30%|███       | 77/256 [00:01<00:04, 44.54it/s]\n",
      " 32%|███▏      | 82/256 [00:01<00:03, 44.22it/s]\n",
      " 34%|███▍      | 87/256 [00:01<00:03, 44.65it/s]\n",
      " 36%|███▌      | 92/256 [00:02<00:03, 43.65it/s]\n",
      " 38%|███▊      | 97/256 [00:02<00:03, 43.68it/s]\n",
      " 40%|███▉      | 102/256 [00:02<00:03, 45.06it/s]\n",
      " 42%|████▏     | 107/256 [00:02<00:03, 45.70it/s]\n",
      " 44%|████▍     | 112/256 [00:02<00:03, 45.75it/s]\n",
      " 46%|████▌     | 117/256 [00:02<00:03, 45.01it/s]\n",
      " 48%|████▊     | 122/256 [00:02<00:03, 41.99it/s]\n",
      " 50%|████▉     | 127/256 [00:02<00:02, 43.21it/s]\n",
      " 52%|█████▏    | 132/256 [00:02<00:02, 42.48it/s]\n",
      " 54%|█████▎    | 137/256 [00:03<00:02, 42.62it/s]\n",
      " 55%|█████▌    | 142/256 [00:03<00:02, 43.65it/s]\n",
      " 57%|█████▋    | 147/256 [00:03<00:02, 42.76it/s]\n",
      " 59%|█████▉    | 152/256 [00:03<00:02, 43.34it/s]\n",
      " 61%|██████▏   | 157/256 [00:03<00:02, 42.55it/s]\n",
      " 63%|██████▎   | 162/256 [00:03<00:02, 40.58it/s]\n",
      " 65%|██████▌   | 167/256 [00:03<00:02, 39.40it/s]\n",
      " 67%|██████▋   | 171/256 [00:03<00:02, 38.95it/s]\n",
      " 69%|██████▉   | 176/256 [00:04<00:01, 41.62it/s]\n",
      " 71%|███████   | 181/256 [00:04<00:01, 43.14it/s]\n",
      " 73%|███████▎  | 186/256 [00:04<00:01, 42.22it/s]\n",
      " 75%|███████▍  | 191/256 [00:04<00:01, 43.19it/s]\n",
      " 77%|███████▋  | 196/256 [00:04<00:01, 43.40it/s]\n",
      " 79%|███████▊  | 201/256 [00:04<00:01, 41.95it/s]\n",
      " 80%|████████  | 206/256 [00:04<00:01, 41.86it/s]\n",
      " 82%|████████▏ | 211/256 [00:04<00:01, 39.89it/s]\n",
      " 84%|████████▍ | 216/256 [00:04<00:00, 41.45it/s]\n",
      " 86%|████████▋ | 221/256 [00:05<00:00, 42.28it/s]\n",
      " 88%|████████▊ | 226/256 [00:05<00:00, 43.79it/s]\n",
      " 90%|█████████ | 231/256 [00:05<00:00, 43.38it/s]\n",
      " 92%|█████████▏| 236/256 [00:05<00:00, 44.30it/s]\n",
      " 94%|█████████▍| 241/256 [00:05<00:00, 43.82it/s]\n",
      " 96%|█████████▌| 246/256 [00:05<00:00, 42.00it/s]\n",
      " 98%|█████████▊| 251/256 [00:05<00:00, 42.19it/s]\n",
      "100%|██████████| 256/256 [00:05<00:00, 42.72it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:27:11 - INFO - src.trainer -   {'eval_loss': 0.4657633304595947, 'eval_acc': 0.7802734375}\n",
      "04/10/2025 15:27:11 - INFO - src.trainer -   Best dev result: 0.7802734375\n",
      "04/10/2025 15:27:13 - INFO - src.trainer -   {'loss': 0.5463495254516602, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 17}\n",
      "04/10/2025 15:27:13 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:27:17 - INFO - __main__ -   *** Validate ***\n",
      "\n",
      "261it [00:12,  2.49it/s]                         \n",
      "265it [00:12,  3.27it/s]\n",
      "270it [00:12,  4.61it/s]\n",
      "275it [00:12,  6.39it/s]\n",
      "280it [00:12,  8.68it/s]\n",
      "285it [00:12, 11.53it/s]\n",
      "290it [00:12, 14.91it/s]\n",
      "295it [00:13, 18.64it/s]\n",
      "300it [00:13, 22.90it/s]\n",
      "305it [00:13, 26.95it/s]\n",
      "310it [00:13, 30.75it/s]\n",
      "315it [00:13, 33.98it/s]\n",
      "320it [00:13, 37.45it/s]\n",
      "325it [00:13, 38.46it/s]\n",
      "330it [00:13, 39.24it/s]\n",
      "335it [00:13, 40.36it/s]\n",
      "340it [00:14, 42.22it/s]\n",
      "345it [00:14, 41.86it/s]\n",
      "350it [00:14, 42.51it/s]\n",
      "355it [00:14, 43.25it/s]\n",
      "360it [00:14, 44.94it/s]\n",
      "365it [00:14, 44.62it/s]\n",
      "370it [00:14, 43.80it/s]\n",
      "375it [00:14, 43.96it/s]\n",
      "380it [00:15, 41.20it/s]\n",
      "385it [00:15, 43.30it/s]\n",
      "390it [00:15, 42.44it/s]\n",
      "395it [00:15, 42.50it/s]\n",
      "400it [00:15, 42.93it/s]\n",
      "405it [00:15, 42.77it/s]\n",
      "410it [00:15, 42.53it/s]\n",
      "415it [00:15, 42.31it/s]\n",
      "420it [00:15, 39.75it/s]\n",
      "425it [00:16, 38.62it/s]\n",
      "430it [00:16, 40.28it/s]\n",
      "435it [00:16, 42.68it/s]\n",
      "440it [00:16, 42.25it/s]\n",
      "445it [00:16, 42.88it/s]\n",
      "450it [00:16, 42.91it/s]\n",
      "455it [00:16, 41.69it/s]\n",
      "460it [00:16, 41.52it/s]\n",
      "465it [00:17, 40.45it/s]\n",
      "470it [00:17, 40.30it/s]\n",
      "475it [00:17, 42.02it/s]\n",
      "480it [00:17, 42.07it/s]\n",
      "485it [00:17, 42.03it/s]\n",
      "490it [00:17, 43.42it/s]\n",
      "495it [00:17, 43.26it/s]\n",
      "500it [00:17, 41.45it/s]\n",
      "505it [00:17, 41.27it/s]\n",
      "510it [00:18, 42.72it/s]04/10/2025 15:27:23 - INFO - src.trainer -   {'eval_loss': 0.4657633304595947, 'eval_acc': 0.7802734375}\n",
      "04/10/2025 15:27:23 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:27:23 - INFO - __main__ -     eval_loss = 0.4657633304595947\n",
      "04/10/2025 15:27:23 - INFO - __main__ -     eval_acc = 0.7802734375\n",
      "04/10/2025 15:27:23 - INFO - root -   *** Test ***\n",
      "\n",
      "515it [00:18, 41.38it/s]\n",
      "520it [00:18, 38.80it/s]\n",
      "525it [00:18, 38.97it/s]\n",
      "529it [00:18, 39.22it/s]\n",
      "533it [00:18, 37.64it/s]\n",
      "537it [00:18, 37.87it/s]\n",
      "541it [00:18, 37.56it/s]\n",
      "545it [00:19, 37.17it/s]\n",
      "549it [00:19, 37.38it/s]\n",
      "553it [00:19, 36.00it/s]\n",
      "558it [00:19, 37.36it/s]\n",
      "562it [00:19, 35.78it/s]\n",
      "566it [00:19, 35.72it/s]\n",
      "570it [00:19, 35.63it/s]\n",
      "574it [00:19, 34.87it/s]\n",
      "578it [00:19, 35.09it/s]\n",
      "582it [00:20, 34.57it/s]\n",
      "586it [00:20, 33.48it/s]\n",
      "590it [00:20, 34.71it/s]\n",
      "595it [00:20, 36.67it/s]\n",
      "599it [00:20, 34.96it/s]\n",
      "603it [00:20, 36.09it/s]\n",
      "607it [00:20, 35.39it/s]\n",
      "611it [00:20, 34.13it/s]\n",
      "615it [00:21, 35.43it/s]\n",
      "619it [00:21, 34.81it/s]\n",
      "623it [00:21, 34.33it/s]\n",
      "627it [00:21, 33.34it/s]\n",
      "631it [00:21, 33.83it/s]\n",
      "635it [00:21, 34.59it/s]\n",
      "640it [00:21, 36.27it/s]\n",
      "644it [00:21, 36.13it/s]\n",
      "648it [00:21, 35.42it/s]\n",
      "652it [00:22, 36.09it/s]\n",
      "656it [00:22, 35.21it/s]\n",
      "660it [00:22, 35.35it/s]\n",
      "664it [00:22, 35.68it/s]\n",
      "668it [00:22, 35.01it/s]\n",
      "673it [00:22, 36.52it/s]\n",
      "677it [00:22, 36.88it/s]\n",
      "681it [00:22, 36.65it/s]\n",
      "685it [00:22, 37.30it/s]\n",
      "689it [00:23, 36.56it/s]\n",
      "693it [00:23, 36.99it/s]\n",
      "697it [00:23, 37.07it/s]\n",
      "701it [00:23, 36.13it/s]\n",
      "705it [00:23, 34.57it/s]\n",
      "709it [00:23, 34.81it/s]\n",
      "713it [00:23, 33.64it/s]\n",
      "717it [00:23, 33.06it/s]\n",
      "721it [00:24, 33.84it/s]\n",
      "725it [00:24, 34.52it/s]\n",
      "729it [00:24, 32.79it/s]04/10/2025 15:27:29 - INFO - src.trainer -   {'eval_loss': 0.3997124433517456, 'eval_acc': 0.8130733944954128}\n",
      "04/10/2025 15:27:29 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:27:29 - INFO - __main__ -     eval_loss = 0.3997124433517456\n",
      "04/10/2025 15:27:29 - INFO - __main__ -     eval_acc = 0.8130733944954128\n",
      "04/10/2025 15:27:29 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:27:29 - INFO - __main__ -   result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2\n",
      "\n",
      "730it [00:24, 30.01it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "------------------------------------Executing command for Client 3 in Round 1:\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-3/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-3/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-3/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-3/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-3/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-3/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 3\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-roberta-large-mezo-ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2025 15:27:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:27:42 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-3/runs/Apr10_15-27-42_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-3,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-3,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=3,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:27:42 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:27:44 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-3/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.007 s]\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-3/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.020 s]\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 5246, 1417, 1635, 8, 11986, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[9], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   text: <s>innovations and glimpse  It was<mask>.</s>\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "04/10/2025 15:27:49 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-3/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.012 s]\n",
      "04/10/2025 15:27:50 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:27:50 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:27:50 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:27:50 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:27:51 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:27:51 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:27:51 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:27:51 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:27:51 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:27:51 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:27:51 - INFO - src.trainer -     Total optimization steps = 10\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:27:53 - INFO - src.trainer -   {'loss': 0.5450965166091919, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 6/256 [00:00<00:04, 54.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 253/256 [00:05<00:00, 45.90it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:28:07 - INFO - src.trainer -   {'eval_loss': 0.4715443253517151, 'eval_acc': 0.7744140625}\n",
      "04/10/2025 15:28:07 - INFO - src.trainer -   Best dev result: 0.7744140625\n",
      "04/10/2025 15:28:09 - INFO - src.trainer -   {'loss': 0.44237327575683594, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 17}\n",
      "04/10/2025 15:28:09 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:28:12 - INFO - __main__ -   *** Validate ***\n",
      "508it [00:16, 45.71it/s]04/10/2025 15:28:18 - INFO - src.trainer -   {'eval_loss': 0.4715443253517151, 'eval_acc': 0.7744140625}\n",
      "04/10/2025 15:28:18 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:28:18 - INFO - __main__ -     eval_loss = 0.4715443253517151\n",
      "04/10/2025 15:28:18 - INFO - __main__ -     eval_acc = 0.7744140625\n",
      "04/10/2025 15:28:18 - INFO - root -   *** Test ***\n",
      "729it [00:23, 32.87it/s]04/10/2025 15:28:24 - INFO - src.trainer -   {'eval_loss': 0.3999873995780945, 'eval_acc': 0.8130733944954128}\n",
      "04/10/2025 15:28:24 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:28:24 - INFO - __main__ -     eval_loss = 0.3999873995780945\n",
      "04/10/2025 15:28:24 - INFO - __main__ -     eval_acc = 0.8130733944954128\n",
      "04/10/2025 15:28:24 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:28:24 - INFO - __main__ -   result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-3\n",
      "730it [00:23, 31.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeZO stdout:\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 3\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-roberta-large-mezo-ft\n",
      "04/10/2025 15:28:30 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:28:30 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-3/runs/Apr10_15-28-30_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-3,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-3,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=3,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:28:30 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:28:32 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "/home/javad/seedfl/medium_models/src/dataset.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.support_examples, self.query_examples = torch.load(cached_features_file)\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-3/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.004 s]\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-3/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.014 s]\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 5246, 1417, 1635, 8, 11986, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[9], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:28:37 - INFO - src.dataset -   text: <s>innovations and glimpse  It was<mask>.</s>\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-3\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-3/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.006 s]\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:28:38 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:28:40 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:28:40 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:28:40 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:28:40 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:28:40 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:28:40 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:28:40 - INFO - src.trainer -     Total optimization steps = 10\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:28:42 - INFO - src.trainer -   {'loss': 0.5450965166091919, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\n",
      "  0%|          | 0/256 [00:00<?, ?it/s]\n",
      "  2%|▏         | 6/256 [00:00<00:05, 50.00it/s]\n",
      "  4%|▍         | 11/256 [00:00<00:05, 44.87it/s]\n",
      "  6%|▋         | 16/256 [00:00<00:05, 44.52it/s]\n",
      "  8%|▊         | 21/256 [00:00<00:05, 43.78it/s]\n",
      " 10%|█         | 26/256 [00:00<00:05, 42.37it/s]\n",
      " 12%|█▏        | 31/256 [00:00<00:05, 42.41it/s]\n",
      " 14%|█▍        | 36/256 [00:00<00:05, 42.57it/s]\n",
      " 16%|█▌        | 41/256 [00:00<00:05, 42.95it/s]\n",
      " 18%|█▊        | 46/256 [00:01<00:04, 43.10it/s]\n",
      " 20%|█▉        | 51/256 [00:01<00:04, 42.93it/s]\n",
      " 22%|██▏       | 56/256 [00:01<00:04, 43.20it/s]\n",
      " 24%|██▍       | 61/256 [00:01<00:04, 43.06it/s]\n",
      " 26%|██▌       | 66/256 [00:01<00:04, 43.54it/s]\n",
      " 28%|██▊       | 71/256 [00:01<00:04, 41.98it/s]\n",
      " 30%|██▉       | 76/256 [00:01<00:04, 41.32it/s]\n",
      " 32%|███▏      | 81/256 [00:01<00:04, 40.11it/s]\n",
      " 34%|███▎      | 86/256 [00:02<00:04, 40.05it/s]\n",
      " 36%|███▌      | 91/256 [00:02<00:04, 41.10it/s]\n",
      " 38%|███▊      | 96/256 [00:02<00:03, 41.66it/s]\n",
      " 39%|███▉      | 101/256 [00:02<00:03, 42.92it/s]\n",
      " 41%|████▏     | 106/256 [00:02<00:03, 42.08it/s]\n",
      " 43%|████▎     | 111/256 [00:02<00:03, 41.41it/s]\n",
      " 45%|████▌     | 116/256 [00:02<00:03, 41.32it/s]\n",
      " 47%|████▋     | 121/256 [00:02<00:03, 42.10it/s]\n",
      " 49%|████▉     | 126/256 [00:02<00:03, 42.04it/s]\n",
      " 51%|█████     | 131/256 [00:03<00:02, 42.03it/s]\n",
      " 53%|█████▎    | 136/256 [00:03<00:02, 41.09it/s]\n",
      " 55%|█████▌    | 141/256 [00:03<00:02, 39.81it/s]\n",
      " 57%|█████▋    | 146/256 [00:03<00:02, 39.86it/s]\n",
      " 59%|█████▉    | 151/256 [00:03<00:02, 40.97it/s]\n",
      " 61%|██████    | 156/256 [00:03<00:02, 39.97it/s]\n",
      " 63%|██████▎   | 161/256 [00:03<00:02, 40.47it/s]\n",
      " 65%|██████▍   | 166/256 [00:03<00:02, 41.52it/s]\n",
      " 67%|██████▋   | 171/256 [00:04<00:02, 42.30it/s]\n",
      " 69%|██████▉   | 176/256 [00:04<00:01, 42.51it/s]\n",
      " 71%|███████   | 181/256 [00:04<00:01, 41.59it/s]\n",
      " 73%|███████▎  | 186/256 [00:04<00:01, 41.60it/s]\n",
      " 75%|███████▍  | 191/256 [00:04<00:01, 41.51it/s]\n",
      " 77%|███████▋  | 196/256 [00:04<00:01, 42.08it/s]\n",
      " 79%|███████▊  | 201/256 [00:04<00:01, 40.89it/s]\n",
      " 80%|████████  | 206/256 [00:04<00:01, 40.10it/s]\n",
      " 82%|████████▏ | 211/256 [00:05<00:01, 40.63it/s]\n",
      " 84%|████████▍ | 216/256 [00:05<00:00, 41.53it/s]\n",
      " 86%|████████▋ | 221/256 [00:05<00:00, 42.59it/s]\n",
      " 88%|████████▊ | 226/256 [00:05<00:00, 41.55it/s]\n",
      " 90%|█████████ | 231/256 [00:05<00:00, 42.19it/s]\n",
      " 92%|█████████▏| 236/256 [00:05<00:00, 40.74it/s]\n",
      " 94%|█████████▍| 241/256 [00:05<00:00, 40.24it/s]\n",
      " 96%|█████████▌| 246/256 [00:05<00:00, 41.73it/s]\n",
      " 98%|█████████▊| 251/256 [00:06<00:00, 42.38it/s]\n",
      "100%|██████████| 256/256 [00:06<00:00, 38.35it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:28:56 - INFO - src.trainer -   {'eval_loss': 0.4715443253517151, 'eval_acc': 0.7744140625}\n",
      "04/10/2025 15:28:56 - INFO - src.trainer -   Best dev result: 0.7744140625\n",
      "04/10/2025 15:28:58 - INFO - src.trainer -   {'loss': 0.44237327575683594, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 18}\n",
      "04/10/2025 15:28:58 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:29:02 - INFO - __main__ -   *** Validate ***\n",
      "\n",
      "260it [00:12,  2.33it/s]                         \n",
      "264it [00:12,  3.09it/s]\n",
      "268it [00:12,  4.13it/s]\n",
      "273it [00:12,  5.89it/s]\n",
      "278it [00:13,  8.14it/s]\n",
      "283it [00:13, 10.92it/s]\n",
      "288it [00:13, 14.22it/s]\n",
      "293it [00:13, 17.95it/s]\n",
      "298it [00:13, 22.03it/s]\n",
      "303it [00:13, 26.11it/s]\n",
      "308it [00:13, 29.74it/s]\n",
      "313it [00:13, 32.43it/s]\n",
      "318it [00:13, 35.19it/s]\n",
      "323it [00:14, 37.08it/s]\n",
      "328it [00:14, 37.57it/s]\n",
      "333it [00:14, 38.26it/s]\n",
      "338it [00:14, 38.41it/s]\n",
      "343it [00:14, 39.05it/s]\n",
      "348it [00:14, 39.93it/s]\n",
      "353it [00:14, 40.06it/s]\n",
      "358it [00:14, 40.60it/s]\n",
      "363it [00:15, 41.17it/s]\n",
      "368it [00:15, 40.71it/s]\n",
      "373it [00:15, 40.90it/s]\n",
      "378it [00:15, 41.33it/s]\n",
      "383it [00:15, 42.03it/s]\n",
      "388it [00:15, 41.24it/s]\n",
      "393it [00:15, 41.13it/s]\n",
      "398it [00:15, 39.33it/s]\n",
      "403it [00:16, 39.49it/s]\n",
      "408it [00:16, 40.51it/s]\n",
      "413it [00:16, 36.63it/s]\n",
      "417it [00:16, 33.88it/s]\n",
      "421it [00:16, 32.23it/s]\n",
      "426it [00:16, 34.57it/s]\n",
      "431it [00:16, 37.21it/s]\n",
      "435it [00:16, 37.60it/s]\n",
      "440it [00:17, 40.18it/s]\n",
      "445it [00:17, 40.21it/s]\n",
      "450it [00:17, 41.21it/s]\n",
      "455it [00:17, 41.22it/s]\n",
      "460it [00:17, 39.22it/s]\n",
      "465it [00:17, 39.92it/s]\n",
      "470it [00:17, 41.55it/s]\n",
      "475it [00:17, 42.16it/s]\n",
      "480it [00:18, 42.06it/s]\n",
      "485it [00:18, 41.82it/s]\n",
      "490it [00:18, 41.82it/s]\n",
      "495it [00:18, 40.10it/s]\n",
      "500it [00:18, 40.86it/s]\n",
      "505it [00:18, 41.18it/s]\n",
      "510it [00:18, 40.45it/s]04/10/2025 15:29:08 - INFO - src.trainer -   {'eval_loss': 0.4715443253517151, 'eval_acc': 0.7744140625}\n",
      "04/10/2025 15:29:08 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:29:08 - INFO - __main__ -     eval_loss = 0.4715443253517151\n",
      "04/10/2025 15:29:08 - INFO - __main__ -     eval_acc = 0.7744140625\n",
      "04/10/2025 15:29:08 - INFO - root -   *** Test ***\n",
      "\n",
      "515it [00:18, 39.09it/s]\n",
      "519it [00:19, 37.45it/s]\n",
      "523it [00:19, 37.25it/s]\n",
      "527it [00:19, 37.87it/s]\n",
      "531it [00:19, 37.88it/s]\n",
      "535it [00:19, 36.93it/s]\n",
      "539it [00:19, 36.32it/s]\n",
      "543it [00:19, 36.66it/s]\n",
      "547it [00:19, 36.03it/s]\n",
      "551it [00:19, 36.62it/s]\n",
      "555it [00:20, 36.02it/s]\n",
      "559it [00:20, 35.87it/s]\n",
      "563it [00:20, 35.80it/s]\n",
      "567it [00:20, 35.58it/s]\n",
      "571it [00:20, 34.63it/s]\n",
      "575it [00:20, 35.24it/s]\n",
      "579it [00:20, 34.09it/s]\n",
      "583it [00:20, 34.31it/s]\n",
      "587it [00:20, 33.12it/s]\n",
      "591it [00:21, 34.79it/s]\n",
      "595it [00:21, 36.13it/s]\n",
      "599it [00:21, 34.44it/s]\n",
      "603it [00:21, 35.59it/s]\n",
      "607it [00:21, 34.96it/s]\n",
      "611it [00:21, 33.71it/s]\n",
      "615it [00:21, 34.93it/s]\n",
      "619it [00:21, 34.44it/s]\n",
      "623it [00:22, 33.97it/s]\n",
      "627it [00:22, 32.91it/s]\n",
      "631it [00:22, 33.48it/s]\n",
      "635it [00:22, 34.22it/s]\n",
      "639it [00:22, 35.69it/s]\n",
      "643it [00:22, 35.72it/s]\n",
      "647it [00:22, 35.05it/s]\n",
      "651it [00:22, 35.73it/s]\n",
      "655it [00:22, 34.81it/s]\n",
      "659it [00:23, 34.94it/s]\n",
      "663it [00:23, 35.24it/s]\n",
      "667it [00:23, 33.38it/s]\n",
      "671it [00:23, 32.41it/s]\n",
      "675it [00:23, 33.41it/s]\n",
      "679it [00:23, 34.69it/s]\n",
      "683it [00:23, 35.05it/s]\n",
      "687it [00:23, 35.60it/s]\n",
      "691it [00:23, 36.05it/s]\n",
      "695it [00:24, 35.63it/s]\n",
      "699it [00:24, 35.46it/s]\n",
      "703it [00:24, 34.11it/s]\n",
      "707it [00:24, 34.49it/s]\n",
      "711it [00:24, 34.06it/s]\n",
      "715it [00:24, 32.30it/s]\n",
      "719it [00:24, 33.36it/s]\n",
      "723it [00:24, 34.07it/s]\n",
      "727it [00:25, 33.67it/s]04/10/2025 15:29:15 - INFO - src.trainer -   {'eval_loss': 0.3999873995780945, 'eval_acc': 0.8130733944954128}\n",
      "04/10/2025 15:29:15 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:29:15 - INFO - __main__ -     eval_loss = 0.3999873995780945\n",
      "04/10/2025 15:29:15 - INFO - __main__ -     eval_acc = 0.8130733944954128\n",
      "04/10/2025 15:29:15 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:29:15 - INFO - __main__ -   result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed3-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-3\n",
      "\n",
      "730it [00:25, 29.02it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "------------------------------------Executing command for Client 4 in Round 1:\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-4/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-4/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-4/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-4/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-4/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-4/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 4\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-roberta-large-mezo-ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2025 15:29:27 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:29:27 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-4/runs/Apr10_15-29-27_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-4,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-4,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=4,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:29:27 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:29:29 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-4/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.007 s]\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-4/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.022 s]\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 102, 33139, 1527, 2849, 40776, 3329, 17070, 2196, 1025, 385, 28627, 15759, 5712, 3269, 2156, 8, 114, 47, 128, 241, 164, 7, 11330, 5, 741, 1120, 128, 29, 3558, 2156, 47, 128, 417, 357, 33, 10, 205, 3626, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=0, mask_pos=[42], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:29:34 - INFO - src.dataset -   text: <s>a baffling subplot involving smuggling drugs inside danish cows falls flat, and if you're going to alter the bard's ending, you 'd better have a good alternative  It was<mask>.</s>\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-4/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.013 s]\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:29:35 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:29:37 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:29:37 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:29:37 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:29:37 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:29:37 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:29:37 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:29:37 - INFO - src.trainer -     Total optimization steps = 10\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:29:39 - INFO - src.trainer -   {'loss': 0.47084397077560425, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 5/256 [00:00<00:05, 48.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 255/256 [00:06<00:00, 40.48it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:29:54 - INFO - src.trainer -   {'eval_loss': 0.4661163091659546, 'eval_acc': 0.7919921875}\n",
      "04/10/2025 15:29:54 - INFO - src.trainer -   Best dev result: 0.7919921875\n",
      "04/10/2025 15:29:56 - INFO - src.trainer -   {'loss': 0.45142635703086853, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 18}\n",
      "04/10/2025 15:29:56 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:30:00 - INFO - __main__ -   *** Validate ***\n",
      "511it [00:18, 41.28it/s]04/10/2025 15:30:06 - INFO - src.trainer -   {'eval_loss': 0.4661163091659546, 'eval_acc': 0.7919921875}\n",
      "04/10/2025 15:30:06 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:30:06 - INFO - __main__ -     eval_loss = 0.4661163091659546\n",
      "04/10/2025 15:30:06 - INFO - __main__ -     eval_acc = 0.7919921875\n",
      "04/10/2025 15:30:06 - INFO - root -   *** Test ***\n",
      "727it [00:25, 31.87it/s]04/10/2025 15:30:12 - INFO - src.trainer -   {'eval_loss': 0.40056735277175903, 'eval_acc': 0.8153669724770642}\n",
      "04/10/2025 15:30:12 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:30:12 - INFO - __main__ -     eval_loss = 0.40056735277175903\n",
      "04/10/2025 15:30:12 - INFO - __main__ -     eval_acc = 0.8153669724770642\n",
      "04/10/2025 15:30:12 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:30:12 - INFO - __main__ -   result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-4\n",
      "730it [00:25, 29.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeZO stdout:\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 4\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-roberta-large-mezo-ft\n",
      "04/10/2025 15:30:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:30:19 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-4/runs/Apr10_15-30-19_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-4,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-4,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=4,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:30:19 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:30:20 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "/home/javad/seedfl/medium_models/src/dataset.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.support_examples, self.query_examples = torch.load(cached_features_file)\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-4/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.004 s]\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-4/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.014 s]\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 102, 33139, 1527, 2849, 40776, 3329, 17070, 2196, 1025, 385, 28627, 15759, 5712, 3269, 2156, 8, 114, 47, 128, 241, 164, 7, 11330, 5, 741, 1120, 128, 29, 3558, 2156, 47, 128, 417, 357, 33, 10, 205, 3626, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=0, mask_pos=[42], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:30:26 - INFO - src.dataset -   text: <s>a baffling subplot involving smuggling drugs inside danish cows falls flat, and if you're going to alter the bard's ending, you 'd better have a good alternative  It was<mask>.</s>\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-4\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-4/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.005 s]\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:30:27 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:30:29 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:30:29 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:30:29 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:30:29 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:30:29 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:30:29 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:30:29 - INFO - src.trainer -     Total optimization steps = 10\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:30:30 - INFO - src.trainer -   {'loss': 0.47084397077560425, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\n",
      "  0%|          | 0/256 [00:00<?, ?it/s]\n",
      "  2%|▏         | 5/256 [00:00<00:05, 49.01it/s]\n",
      "  4%|▍         | 10/256 [00:00<00:05, 46.32it/s]\n",
      "  6%|▌         | 15/256 [00:00<00:05, 44.93it/s]\n",
      "  8%|▊         | 20/256 [00:00<00:05, 42.85it/s]\n",
      " 10%|▉         | 25/256 [00:00<00:05, 43.54it/s]\n",
      " 12%|█▏        | 30/256 [00:00<00:05, 43.05it/s]\n",
      " 14%|█▎        | 35/256 [00:00<00:05, 43.71it/s]\n",
      " 16%|█▌        | 40/256 [00:00<00:05, 41.68it/s]\n",
      " 18%|█▊        | 45/256 [00:01<00:05, 41.26it/s]\n",
      " 20%|█▉        | 50/256 [00:01<00:04, 41.44it/s]\n",
      " 21%|██▏       | 55/256 [00:01<00:04, 41.31it/s]\n",
      " 23%|██▎       | 60/256 [00:01<00:04, 41.96it/s]\n",
      " 25%|██▌       | 65/256 [00:01<00:04, 42.74it/s]\n",
      " 27%|██▋       | 70/256 [00:01<00:04, 41.51it/s]\n",
      " 29%|██▉       | 75/256 [00:01<00:04, 41.03it/s]\n",
      " 31%|███▏      | 80/256 [00:01<00:04, 42.34it/s]\n",
      " 33%|███▎      | 85/256 [00:01<00:03, 42.97it/s]\n",
      " 35%|███▌      | 90/256 [00:02<00:03, 43.39it/s]\n",
      " 37%|███▋      | 95/256 [00:02<00:03, 43.53it/s]\n",
      " 39%|███▉      | 100/256 [00:02<00:03, 41.90it/s]\n",
      " 41%|████      | 105/256 [00:02<00:03, 40.22it/s]\n",
      " 43%|████▎     | 110/256 [00:02<00:03, 40.00it/s]\n",
      " 45%|████▍     | 115/256 [00:02<00:03, 40.86it/s]\n",
      " 47%|████▋     | 120/256 [00:02<00:03, 41.67it/s]\n",
      " 49%|████▉     | 125/256 [00:02<00:03, 42.38it/s]\n",
      " 51%|█████     | 130/256 [00:03<00:02, 42.63it/s]\n",
      " 53%|█████▎    | 135/256 [00:03<00:02, 43.01it/s]\n",
      " 55%|█████▍    | 140/256 [00:03<00:02, 41.75it/s]\n",
      " 57%|█████▋    | 145/256 [00:03<00:02, 41.82it/s]\n",
      " 59%|█████▊    | 150/256 [00:03<00:02, 42.28it/s]\n",
      " 61%|██████    | 155/256 [00:03<00:02, 43.21it/s]\n",
      " 62%|██████▎   | 160/256 [00:03<00:02, 41.31it/s]\n",
      " 64%|██████▍   | 165/256 [00:03<00:02, 42.30it/s]\n",
      " 66%|██████▋   | 170/256 [00:04<00:02, 42.06it/s]\n",
      " 68%|██████▊   | 175/256 [00:04<00:01, 43.17it/s]\n",
      " 70%|███████   | 180/256 [00:04<00:01, 41.89it/s]\n",
      " 72%|███████▏  | 185/256 [00:04<00:01, 42.09it/s]\n",
      " 74%|███████▍  | 190/256 [00:04<00:01, 42.97it/s]\n",
      " 76%|███████▌  | 195/256 [00:04<00:01, 40.33it/s]\n",
      " 78%|███████▊  | 200/256 [00:04<00:01, 39.08it/s]\n",
      " 80%|███████▉  | 204/256 [00:04<00:01, 39.12it/s]\n",
      " 82%|████████▏ | 209/256 [00:04<00:01, 39.75it/s]\n",
      " 84%|████████▎ | 214/256 [00:05<00:01, 41.12it/s]\n",
      " 86%|████████▌ | 219/256 [00:05<00:00, 41.86it/s]\n",
      " 88%|████████▊ | 224/256 [00:05<00:00, 38.94it/s]\n",
      " 89%|████████▉ | 229/256 [00:05<00:00, 41.31it/s]\n",
      " 91%|█████████▏| 234/256 [00:05<00:00, 42.37it/s]\n",
      " 93%|█████████▎| 239/256 [00:05<00:00, 40.38it/s]\n",
      " 95%|█████████▌| 244/256 [00:05<00:00, 41.04it/s]\n",
      " 97%|█████████▋| 249/256 [00:05<00:00, 42.10it/s]\n",
      " 99%|█████████▉| 254/256 [00:06<00:00, 42.70it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:30:45 - INFO - src.trainer -   {'eval_loss': 0.4661163091659546, 'eval_acc': 0.7919921875}\n",
      "04/10/2025 15:30:45 - INFO - src.trainer -   Best dev result: 0.7919921875\n",
      "04/10/2025 15:30:47 - INFO - src.trainer -   {'loss': 0.45142635703086853, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 18}\n",
      "04/10/2025 15:30:47 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:30:51 - INFO - __main__ -   *** Validate ***\n",
      "\n",
      "259it [00:12,  2.52it/s]                         \n",
      "263it [00:12,  3.30it/s]\n",
      "268it [00:12,  4.64it/s]\n",
      "273it [00:12,  6.41it/s]\n",
      "278it [00:12,  8.65it/s]\n",
      "283it [00:12, 11.46it/s]\n",
      "288it [00:13, 14.69it/s]\n",
      "293it [00:13, 18.17it/s]\n",
      "298it [00:13, 21.70it/s]\n",
      "303it [00:13, 25.73it/s]\n",
      "308it [00:13, 29.70it/s]\n",
      "313it [00:13, 32.94it/s]\n",
      "318it [00:13, 36.08it/s]\n",
      "323it [00:13, 38.69it/s]\n",
      "328it [00:14, 38.29it/s]\n",
      "333it [00:14, 39.31it/s]\n",
      "338it [00:14, 40.78it/s]\n",
      "343it [00:14, 41.02it/s]\n",
      "348it [00:14, 41.85it/s]\n",
      "353it [00:14, 42.49it/s]\n",
      "358it [00:14, 40.98it/s]\n",
      "363it [00:14, 39.67it/s]\n",
      "368it [00:15, 39.74it/s]\n",
      "373it [00:15, 41.12it/s]\n",
      "378it [00:15, 41.99it/s]\n",
      "383it [00:15, 41.80it/s]\n",
      "388it [00:15, 41.90it/s]\n",
      "393it [00:15, 42.13it/s]\n",
      "398it [00:15, 40.57it/s]\n",
      "403it [00:15, 41.62it/s]\n",
      "408it [00:15, 42.26it/s]\n",
      "413it [00:16, 41.34it/s]\n",
      "418it [00:16, 42.24it/s]\n",
      "423it [00:16, 42.39it/s]\n",
      "428it [00:16, 42.18it/s]\n",
      "433it [00:16, 41.91it/s]\n",
      "438it [00:16, 41.82it/s]\n",
      "443it [00:16, 42.41it/s]\n",
      "448it [00:16, 42.25it/s]\n",
      "453it [00:17, 40.71it/s]\n",
      "458it [00:17, 39.75it/s]\n",
      "462it [00:17, 39.43it/s]\n",
      "467it [00:17, 41.28it/s]\n",
      "472it [00:17, 41.84it/s]\n",
      "477it [00:17, 40.91it/s]\n",
      "482it [00:17, 40.02it/s]\n",
      "487it [00:17, 41.69it/s]\n",
      "492it [00:17, 40.27it/s]\n",
      "497it [00:18, 40.00it/s]\n",
      "502it [00:18, 40.70it/s]\n",
      "507it [00:18, 42.07it/s]\n",
      "512it [00:18, 42.63it/s]04/10/2025 15:30:57 - INFO - src.trainer -   {'eval_loss': 0.4661163091659546, 'eval_acc': 0.7919921875}\n",
      "04/10/2025 15:30:57 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:30:57 - INFO - __main__ -     eval_loss = 0.4661163091659546\n",
      "04/10/2025 15:30:57 - INFO - __main__ -     eval_acc = 0.7919921875\n",
      "04/10/2025 15:30:57 - INFO - root -   *** Test ***\n",
      "\n",
      "517it [00:18, 39.58it/s]\n",
      "522it [00:18, 38.18it/s]\n",
      "527it [00:18, 38.85it/s]\n",
      "531it [00:18, 38.50it/s]\n",
      "535it [00:19, 37.43it/s]\n",
      "539it [00:19, 36.68it/s]\n",
      "543it [00:19, 36.97it/s]\n",
      "547it [00:19, 36.34it/s]\n",
      "551it [00:19, 36.83it/s]\n",
      "555it [00:19, 36.18it/s]\n",
      "559it [00:19, 35.94it/s]\n",
      "563it [00:19, 35.83it/s]\n",
      "567it [00:19, 35.51it/s]\n",
      "571it [00:20, 34.49it/s]\n",
      "575it [00:20, 35.23it/s]\n",
      "579it [00:20, 34.04it/s]\n",
      "583it [00:20, 34.31it/s]\n",
      "587it [00:20, 33.19it/s]\n",
      "591it [00:20, 34.91it/s]\n",
      "595it [00:20, 36.16it/s]\n",
      "599it [00:20, 34.41it/s]\n",
      "603it [00:21, 35.55it/s]\n",
      "607it [00:21, 33.91it/s]\n",
      "611it [00:21, 31.79it/s]\n",
      "615it [00:21, 32.04it/s]\n",
      "619it [00:21, 32.29it/s]\n",
      "623it [00:21, 32.35it/s]\n",
      "627it [00:21, 31.89it/s]\n",
      "631it [00:21, 32.69it/s]\n",
      "635it [00:22, 33.62it/s]\n",
      "639it [00:22, 35.17it/s]\n",
      "643it [00:22, 35.31it/s]\n",
      "647it [00:22, 34.67it/s]\n",
      "651it [00:22, 35.43it/s]\n",
      "655it [00:22, 34.58it/s]\n",
      "659it [00:22, 34.77it/s]\n",
      "663it [00:22, 35.06it/s]\n",
      "667it [00:22, 34.50it/s]\n",
      "671it [00:23, 35.92it/s]\n",
      "675it [00:23, 35.76it/s]\n",
      "679it [00:23, 36.42it/s]\n",
      "683it [00:23, 36.08it/s]\n",
      "687it [00:23, 36.41it/s]\n",
      "691it [00:23, 36.63it/s]\n",
      "695it [00:23, 36.14it/s]\n",
      "699it [00:23, 35.73it/s]\n",
      "703it [00:23, 34.24it/s]\n",
      "707it [00:24, 34.59it/s]\n",
      "711it [00:24, 34.07it/s]\n",
      "715it [00:24, 32.38it/s]\n",
      "719it [00:24, 33.35it/s]\n",
      "723it [00:24, 34.07it/s]\n",
      "727it [00:24, 33.73it/s]04/10/2025 15:31:03 - INFO - src.trainer -   {'eval_loss': 0.40056735277175903, 'eval_acc': 0.8153669724770642}\n",
      "04/10/2025 15:31:03 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:31:03 - INFO - __main__ -     eval_loss = 0.40056735277175903\n",
      "04/10/2025 15:31:03 - INFO - __main__ -     eval_acc = 0.8153669724770642\n",
      "04/10/2025 15:31:03 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:31:03 - INFO - __main__ -   result/SST-2-roberta-large-prompt-standard-k512-roberta-large-mezo-ftseed4-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-4\n",
      "\n",
      "730it [00:24, 29.47it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Averaging weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model saved to saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Text: This movie is great! It was <mask>.\n",
      "Input IDs: tensor([[    0,   713,  1569,    16,   372,   328,    85,    21, 50264,     4,\n",
      "             2]], device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 8\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 11, 50265])\n",
      "Predicted: positive, Expected: positive\n",
      "Logits - great: 59.85023498535156, terrible: 55.80278396606445\n",
      "Text: I hated it. It was <mask>.\n",
      "Input IDs: tensor([[    0,   100, 19975,    24,     4,    85,    21, 50264,     4,     2]],\n",
      "       device='cuda:0')\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Mask Position: 7\n",
      "Logits Shape: torch.Size([1, 2])\n",
      "Corrected Logits Shape: torch.Size([1, 10, 50265])\n",
      "Predicted: negative, Expected: negative\n",
      "Logits - great: 55.96430206298828, terrible: 59.89347839355469\n",
      "Evaluation accuracy for saved_model: 100.0%\n",
      "Starting Round 2\n",
      "------------------------------------Executing command for Client 1 in Round 2:\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-1/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-1/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-1/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-1/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-1/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-1/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 1\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-saved_model-mezo-ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2025 15:31:31 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:31:31 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1/runs/Apr10_15-31-31_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=1,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:31:31 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:31:33 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-1/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.008 s]\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-1/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.014 s]\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 354, 1085, 53, 33750, 29098, 43848, 5739, 31, 386, 7, 2073, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=0, mask_pos=[15], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:31:37 - INFO - src.dataset -   text: <s>is nothing but boilerplate clichés from start to finish  It was<mask>.</s>\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-1/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.225 s]\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:31:38 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:31:40 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:31:40 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:31:40 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:31:40 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:31:40 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:31:40 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:31:40 - INFO - src.trainer -     Total optimization steps = 10\n",
      "04/10/2025 15:31:40 - INFO - src.trainer -     Starting fine-tuning.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:31:41 - INFO - src.trainer -   {'loss': 0.4540715515613556, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 5/256 [00:00<00:05, 49.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 255/256 [00:05<00:00, 43.82it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:31:56 - INFO - src.trainer -   {'eval_loss': 0.492828905582428, 'eval_acc': 0.7548828125}\n",
      "04/10/2025 15:31:56 - INFO - src.trainer -   Best dev result: 0.7548828125\n",
      "04/10/2025 15:31:58 - INFO - src.trainer -   {'loss': 0.5371984243392944, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 18}\n",
      "04/10/2025 15:31:58 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:32:01 - INFO - __main__ -   *** Validate ***\n",
      "510it [00:17, 44.03it/s]04/10/2025 15:32:07 - INFO - src.trainer -   {'eval_loss': 0.492828905582428, 'eval_acc': 0.7548828125}\n",
      "04/10/2025 15:32:07 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:32:07 - INFO - __main__ -     eval_loss = 0.492828905582428\n",
      "04/10/2025 15:32:07 - INFO - __main__ -     eval_acc = 0.7548828125\n",
      "04/10/2025 15:32:07 - INFO - root -   *** Test ***\n",
      "729it [00:23, 32.65it/s]04/10/2025 15:32:13 - INFO - src.trainer -   {'eval_loss': 0.3891185224056244, 'eval_acc': 0.8279816513761468}\n",
      "04/10/2025 15:32:13 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:32:13 - INFO - __main__ -     eval_loss = 0.3891185224056244\n",
      "04/10/2025 15:32:13 - INFO - __main__ -     eval_acc = 0.8279816513761468\n",
      "04/10/2025 15:32:13 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:32:13 - INFO - __main__ -   result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1\n",
      "730it [00:23, 30.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeZO stdout:\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 1\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-saved_model-mezo-ft\n",
      "04/10/2025 15:32:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:32:19 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1/runs/Apr10_15-32-19_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=1,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:32:19 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:32:20 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "/home/javad/seedfl/medium_models/src/dataset.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.support_examples, self.query_examples = torch.load(cached_features_file)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-1/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.004 s]\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-1/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.013 s]\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 354, 1085, 53, 33750, 29098, 43848, 5739, 31, 386, 7, 2073, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=0, mask_pos=[15], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   text: <s>is nothing but boilerplate clichés from start to finish  It was<mask>.</s>\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-1\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/512-1/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.012 s]\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:32:25 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:32:27 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:32:27 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:32:27 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:32:27 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:32:27 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:32:27 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:32:27 - INFO - src.trainer -     Total optimization steps = 10\n",
      "04/10/2025 15:32:27 - INFO - src.trainer -     Starting fine-tuning.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:32:29 - INFO - src.trainer -   {'loss': 0.4540715515613556, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "\n",
      "  0%|          | 0/256 [00:00<?, ?it/s]\n",
      "  2%|▏         | 6/256 [00:00<00:05, 49.42it/s]\n",
      "  4%|▍         | 11/256 [00:00<00:05, 47.08it/s]\n",
      "  6%|▋         | 16/256 [00:00<00:05, 45.52it/s]\n",
      "  8%|▊         | 21/256 [00:00<00:05, 42.22it/s]\n",
      " 10%|█         | 26/256 [00:00<00:05, 41.91it/s]\n",
      " 12%|█▏        | 31/256 [00:00<00:05, 41.16it/s]\n",
      " 14%|█▍        | 36/256 [00:00<00:05, 41.81it/s]\n",
      " 16%|█▌        | 41/256 [00:00<00:04, 43.00it/s]\n",
      " 18%|█▊        | 46/256 [00:01<00:04, 43.22it/s]\n",
      " 20%|█▉        | 51/256 [00:01<00:04, 44.09it/s]\n",
      " 22%|██▏       | 56/256 [00:01<00:04, 43.36it/s]\n",
      " 24%|██▍       | 61/256 [00:01<00:04, 43.23it/s]\n",
      " 26%|██▌       | 66/256 [00:01<00:04, 40.85it/s]\n",
      " 28%|██▊       | 71/256 [00:01<00:04, 41.07it/s]\n",
      " 30%|██▉       | 76/256 [00:01<00:04, 41.71it/s]\n",
      " 32%|███▏      | 81/256 [00:01<00:04, 41.29it/s]\n",
      " 34%|███▎      | 86/256 [00:02<00:04, 41.89it/s]\n",
      " 36%|███▌      | 92/256 [00:02<00:03, 44.26it/s]\n",
      " 38%|███▊      | 97/256 [00:02<00:03, 44.96it/s]\n",
      " 40%|███▉      | 102/256 [00:02<00:03, 43.18it/s]\n",
      " 42%|████▏     | 107/256 [00:02<00:03, 44.11it/s]\n",
      " 44%|████▍     | 112/256 [00:02<00:03, 43.08it/s]\n",
      " 46%|████▌     | 117/256 [00:02<00:03, 42.36it/s]\n",
      " 48%|████▊     | 122/256 [00:02<00:03, 42.78it/s]\n",
      " 50%|████▉     | 127/256 [00:02<00:03, 41.26it/s]\n",
      " 52%|█████▏    | 132/256 [00:03<00:02, 41.91it/s]\n",
      " 54%|█████▎    | 137/256 [00:03<00:02, 43.03it/s]\n",
      " 55%|█████▌    | 142/256 [00:03<00:02, 43.34it/s]\n",
      " 57%|█████▋    | 147/256 [00:03<00:02, 43.14it/s]\n",
      " 59%|█████▉    | 152/256 [00:03<00:02, 44.18it/s]\n",
      " 61%|██████▏   | 157/256 [00:03<00:02, 44.89it/s]\n",
      " 64%|██████▎   | 163/256 [00:03<00:02, 46.46it/s]\n",
      " 66%|██████▌   | 168/256 [00:03<00:01, 46.13it/s]\n",
      " 68%|██████▊   | 173/256 [00:03<00:01, 47.09it/s]\n",
      " 70%|██████▉   | 178/256 [00:04<00:01, 46.05it/s]\n",
      " 71%|███████▏  | 183/256 [00:04<00:01, 43.08it/s]\n",
      " 74%|███████▍  | 189/256 [00:04<00:01, 45.42it/s]\n",
      " 76%|███████▌  | 194/256 [00:04<00:01, 45.54it/s]\n",
      " 78%|███████▊  | 199/256 [00:04<00:01, 43.24it/s]\n",
      " 80%|███████▉  | 204/256 [00:04<00:01, 42.37it/s]\n",
      " 82%|████████▏ | 209/256 [00:04<00:01, 44.05it/s]\n",
      " 84%|████████▎ | 214/256 [00:04<00:00, 44.19it/s]\n",
      " 86%|████████▌ | 219/256 [00:05<00:00, 45.07it/s]\n",
      " 88%|████████▊ | 225/256 [00:05<00:00, 46.81it/s]\n",
      " 90%|████████▉ | 230/256 [00:05<00:00, 45.87it/s]\n",
      " 92%|█████████▏| 235/256 [00:05<00:00, 43.93it/s]\n",
      " 94%|█████████▍| 240/256 [00:05<00:00, 43.77it/s]\n",
      " 96%|█████████▌| 246/256 [00:05<00:00, 45.84it/s]\n",
      " 98%|█████████▊| 251/256 [00:05<00:00, 45.21it/s]\n",
      "100%|██████████| 256/256 [00:05<00:00, 44.77it/s]/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "04/10/2025 15:32:43 - INFO - src.trainer -   {'eval_loss': 0.492828905582428, 'eval_acc': 0.7548828125}\n",
      "04/10/2025 15:32:43 - INFO - src.trainer -   Best dev result: 0.7548828125\n",
      "04/10/2025 15:32:45 - INFO - src.trainer -   {'loss': 0.5371984243392944, 'learning_rate': 1e-06, 'global_step': 10, 'zo_forward_step': 22, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 18}\n",
      "04/10/2025 15:32:45 - INFO - src.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "04/10/2025 15:32:49 - INFO - __main__ -   *** Validate ***\n",
      "\n",
      "261it [00:12,  2.63it/s]                         \n",
      "266it [00:12,  3.64it/s]\n",
      "271it [00:12,  4.99it/s]\n",
      "276it [00:12,  6.72it/s]\n",
      "280it [00:12,  8.51it/s]\n",
      "284it [00:12, 10.62it/s]\n",
      "289it [00:12, 14.03it/s]\n",
      "294it [00:12, 18.07it/s]\n",
      "299it [00:12, 22.05it/s]\n",
      "304it [00:13, 26.00it/s]\n",
      "309it [00:13, 30.09it/s]\n",
      "314it [00:13, 33.06it/s]\n",
      "319it [00:13, 34.59it/s]\n",
      "324it [00:13, 36.01it/s]\n",
      "329it [00:13, 36.80it/s]\n",
      "334it [00:13, 37.98it/s]\n",
      "339it [00:13, 38.63it/s]\n",
      "345it [00:14, 41.90it/s]\n",
      "350it [00:14, 43.79it/s]\n",
      "355it [00:14, 43.57it/s]\n",
      "360it [00:14, 42.92it/s]\n",
      "365it [00:14, 43.11it/s]\n",
      "370it [00:14, 42.43it/s]\n",
      "375it [00:14, 41.72it/s]\n",
      "380it [00:14, 41.60it/s]\n",
      "385it [00:15, 41.83it/s]\n",
      "390it [00:15, 42.15it/s]\n",
      "395it [00:15, 42.60it/s]\n",
      "400it [00:15, 43.44it/s]\n",
      "405it [00:15, 42.83it/s]\n",
      "410it [00:15, 43.59it/s]\n",
      "415it [00:15, 44.21it/s]\n",
      "420it [00:15, 45.21it/s]\n",
      "425it [00:15, 45.38it/s]\n",
      "430it [00:16, 46.38it/s]\n",
      "435it [00:16, 43.97it/s]\n",
      "440it [00:16, 43.09it/s]\n",
      "445it [00:16, 44.83it/s]\n",
      "450it [00:16, 44.96it/s]\n",
      "455it [00:16, 42.53it/s]\n",
      "460it [00:16, 41.87it/s]\n",
      "465it [00:16, 43.87it/s]\n",
      "470it [00:16, 43.89it/s]\n",
      "475it [00:17, 44.69it/s]\n",
      "480it [00:17, 46.00it/s]\n",
      "485it [00:17, 45.32it/s]\n",
      "490it [00:17, 43.73it/s]\n",
      "495it [00:17, 43.64it/s]\n",
      "500it [00:17, 45.24it/s]\n",
      "505it [00:17, 46.24it/s]\n",
      "510it [00:17, 44.83it/s]04/10/2025 15:32:55 - INFO - src.trainer -   {'eval_loss': 0.492828905582428, 'eval_acc': 0.7548828125}\n",
      "04/10/2025 15:32:55 - INFO - __main__ -   ***** Eval results sst-2 *****\n",
      "04/10/2025 15:32:55 - INFO - __main__ -     eval_loss = 0.492828905582428\n",
      "04/10/2025 15:32:55 - INFO - __main__ -     eval_acc = 0.7548828125\n",
      "04/10/2025 15:32:55 - INFO - root -   *** Test ***\n",
      "\n",
      "515it [00:17, 42.85it/s]\n",
      "520it [00:18, 39.73it/s]\n",
      "525it [00:18, 39.63it/s]\n",
      "530it [00:18, 39.58it/s]\n",
      "534it [00:18, 38.40it/s]\n",
      "538it [00:18, 38.39it/s]\n",
      "542it [00:18, 37.63it/s]\n",
      "546it [00:18, 36.32it/s]\n",
      "551it [00:18, 37.50it/s]\n",
      "555it [00:19, 36.72it/s]\n",
      "559it [00:19, 36.35it/s]\n",
      "563it [00:19, 36.24it/s]\n",
      "567it [00:19, 36.08it/s]\n",
      "571it [00:19, 35.13it/s]\n",
      "575it [00:19, 35.84it/s]\n",
      "579it [00:19, 34.64it/s]\n",
      "583it [00:19, 34.91it/s]\n",
      "587it [00:19, 33.73it/s]\n",
      "592it [00:20, 35.65it/s]\n",
      "596it [00:20, 35.87it/s]\n",
      "600it [00:20, 35.19it/s]\n",
      "604it [00:20, 36.13it/s]\n",
      "608it [00:20, 35.30it/s]\n",
      "612it [00:20, 34.58it/s]\n",
      "616it [00:20, 35.15it/s]\n",
      "620it [00:20, 35.13it/s]\n",
      "624it [00:21, 33.98it/s]\n",
      "628it [00:21, 32.97it/s]\n",
      "632it [00:21, 33.77it/s]\n",
      "636it [00:21, 34.39it/s]\n",
      "641it [00:21, 35.42it/s]\n",
      "645it [00:21, 35.40it/s]\n",
      "649it [00:21, 35.73it/s]\n",
      "653it [00:21, 36.19it/s]\n",
      "657it [00:21, 35.93it/s]\n",
      "661it [00:22, 35.29it/s]\n",
      "665it [00:22, 35.69it/s]\n",
      "669it [00:22, 35.51it/s]\n",
      "673it [00:22, 36.25it/s]\n",
      "677it [00:22, 36.75it/s]\n",
      "681it [00:22, 36.52it/s]\n",
      "685it [00:22, 37.08it/s]\n",
      "689it [00:22, 36.44it/s]\n",
      "693it [00:22, 36.97it/s]\n",
      "697it [00:23, 37.08it/s]\n",
      "701it [00:23, 36.07it/s]\n",
      "705it [00:23, 34.55it/s]\n",
      "709it [00:23, 34.81it/s]\n",
      "713it [00:23, 33.56it/s]\n",
      "717it [00:23, 32.94it/s]\n",
      "721it [00:23, 33.76it/s]\n",
      "725it [00:23, 34.47it/s]\n",
      "729it [00:24, 32.80it/s]04/10/2025 15:33:02 - INFO - src.trainer -   {'eval_loss': 0.3891185224056244, 'eval_acc': 0.8279816513761468}\n",
      "04/10/2025 15:33:02 - INFO - __main__ -   ***** Test results sst-2 *****\n",
      "04/10/2025 15:33:02 - INFO - __main__ -     eval_loss = 0.3891185224056244\n",
      "04/10/2025 15:33:02 - INFO - __main__ -     eval_acc = 0.8279816513761468\n",
      "04/10/2025 15:33:02 - INFO - __main__ -   ****** Output Dir *******\n",
      "04/10/2025 15:33:02 - INFO - __main__ -   result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed1-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-1\n",
      "\n",
      "730it [00:24, 30.34it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"output_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "------------------------------------Executing command for Client 2 in Round 2:\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_train_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_test_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_dev_RobertaTokenizerFast-roberta_128_sst-2.lock\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_dev_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_train_RobertaTokenizerFast-roberta_128_sst-2\n",
      "Deleted: data/k-shot-1k-test/SST-2/512-2/cached_test_RobertaTokenizerFast-roberta_128_sst-2\n",
      "TASK: SST-2\n",
      "K: 512\n",
      "Seed: 2\n",
      "BS: 64\n",
      "LR: 1e-6\n",
      "EPS: 1e-3\n",
      "Step: 10; Eval step: 10\n",
      "Grid search tag: seed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10\n",
      "Tag: k512-saved_model-mezo-ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/10/2025 15:33:13 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "04/10/2025 15:33:13 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "adjust_for_init=False,\n",
      "array_id=-1,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "binary_classification=False,\n",
      "change_grad_estimate=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "efficient_zero_order=True,\n",
      "efficient_zero_order_fp16=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluate_during_training=True,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "exclude_embeddings=False,\n",
      "exclude_first_layers=-1,\n",
      "exclude_head=False,\n",
      "f0_scaling=1.0,\n",
      "fix_layers=0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "from_linearhead=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "head_tuning=False,\n",
      "hf_inference_model=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "kernel_formula=sgd,\n",
      "kernel_gamma=1.0,\n",
      "kernel_regularization=0.0,\n",
      "kernel_solver=logistic,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "layer_wise_optim=False,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "load_kernels=None,\n",
      "local_rank=-1,\n",
      "log_file=log,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2/runs/Apr10_15-33-13_2ac9395b7312,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lp_early_stopping=False,\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=10,\n",
      "max_zo_forward_steps=0,\n",
      "mc_tol=0.1,\n",
      "metric_for_best_model=None,\n",
      "model_id=-1,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "no_predict=False,\n",
      "no_reparam=False,\n",
      "no_train=False,\n",
      "norm_running_update=False,\n",
      "num_hvp_vecs=128,\n",
      "num_prefix=10,\n",
      "num_train_epochs=3.0,\n",
      "only_biases=False,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "optimize_acc=False,\n",
      "optimizer=sgd,\n",
      "optimizer_variant=,\n",
      "output_dir=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2,\n",
      "overwrite_kernels=False,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "prefix_init_by_real_act=False,\n",
      "prefix_tuning=False,\n",
      "prob_as_feature=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "random_model_init=False,\n",
      "ray_scope=last,\n",
      "recompute_norms=False,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=result/SST-2-saved_model-prompt-standard-k512-saved_model-mezo-ftseed2-bs64-lr1e-6-eps1e-3-wd0-step10-evalstep10/512-2,\n",
      "save_at_last=False,\n",
      "save_logit=False,\n",
      "save_logit_dir=None,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "scale_lr_with_samples=False,\n",
      "scale_norm_by_num_params=False,\n",
      "seed=2,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sweep=False,\n",
      "sync_embedding_layers=False,\n",
      "tf32=None,\n",
      "tie_emb=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trainer=standard,\n",
      "untie_emb=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_zo_grad_est=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      "zero_order_clip_grad=False,\n",
      "zero_order_eps=0.001,\n",
      "zero_order_optim=True,\n",
      "zero_order_sample=1,\n",
      "zero_order_sample_scheduler=None,\n",
      "zero_order_use_trainer_optim=False,\n",
      "zo_by_layer=False,\n",
      "zo_variant=None,\n",
      ")\n",
      "04/10/2025 15:33:13 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "04/10/2025 15:33:14 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Total num_sample for mode train: 1\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-2/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.007 s]\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Total num_sample for mode dev: 1\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-2/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.014 s]\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   guid: dev-1\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 20042, 1302, 2542, 9, 39, 308, 3035, 1825, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[12], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:33:19 - INFO - src.dataset -   text: <s>never seems aware of his own coolness  It was<mask>.</s>\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   Label 1 to word Ġgreat (372)\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   Total num_sample for mode test: 1\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   Creating features from dataset file at data/k-shot-1k-test/SST-2/512-2\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   Saving features into cached file data/k-shot-1k-test/SST-2/512-2/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.222 s]\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   *** Example ***\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   guid: test-1\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)\n",
      "04/10/2025 15:33:20 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>\n",
      "04/10/2025 15:33:22 - INFO - src.trainer -   ***** Running training *****\n",
      "04/10/2025 15:33:22 - INFO - src.trainer -     Num examples = 1024\n",
      "04/10/2025 15:33:22 - INFO - src.trainer -     Num Epochs = 1\n",
      "04/10/2025 15:33:22 - INFO - src.trainer -     Instantaneous batch size per device = 64\n",
      "04/10/2025 15:33:22 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/10/2025 15:33:22 - INFO - src.trainer -     Gradient Accumulation steps = 1\n",
      "04/10/2025 15:33:22 - INFO - src.trainer -     Total optimization steps = 10\n",
      "04/10/2025 15:33:22 - INFO - src.trainer -     Starting fine-tuning.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "04/10/2025 15:33:24 - INFO - src.trainer -   {'loss': 0.5366418957710266, 'learning_rate': 1e-06, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 10, 'max_zo_forward_steps': 0, 'time': 1}\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 6/256 [00:00<00:04, 50.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_model\",\n",
      "  \"apply_lora\": false,\n",
      "  \"architectures\": [\n",
      "    \"RobertaModelForPromptFinetuning\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"lora_alpha\": null,\n",
      "  \"lora_r\": null,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 171/256 [00:03<00:02, 38.39it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "import subprocess\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class DynamicTrainingArguments:\n",
    "    sfc: bool = False\n",
    "    use_task_word: bool = False\n",
    "    num_labels: int = 2\n",
    "    binary_classification: bool = True\n",
    "    ub: float = 1.0\n",
    "    lb: float = 0.0\n",
    "\n",
    "try:\n",
    "    from src.models import RobertaModelForPromptFinetuning\n",
    "except ImportError:\n",
    "    from transformers import RobertaForSequenceClassification as RobertaModelForPromptFinetuning\n",
    "    print(\"Warning: Using fallback model.\")\n",
    "\n",
    "def evaluate_model(model_dir):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "    model = RobertaModelForPromptFinetuning.from_pretrained(model_dir)\n",
    "    model.model_args = DynamicTrainingArguments()\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    test_samples = [\n",
    "        {\"text\": \"This movie is great! It was\", \"label\": \"positive\"},\n",
    "        {\"text\": \"I hated it. It was\", \"label\": \"negative\"}\n",
    "    ]\n",
    "    label_words = {\"positive\": \"Ġgreat\", \"negative\": \"Ġterrible\"}\n",
    "    label_ids = {label: tokenizer.convert_tokens_to_ids(word) for label, word in label_words.items()}\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in test_samples:\n",
    "            text_with_mask = f\"{sample['text']} <mask>.\"\n",
    "            inputs = tokenizer(text_with_mask, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "            print(f\"Text: {text_with_mask}\")\n",
    "            print(f\"Input IDs: {input_ids}\")\n",
    "            print(f\"Attention Mask: {attention_mask}\")\n",
    "            mask_pos = torch.where(input_ids[0] == tokenizer.mask_token_id)[0].item()\n",
    "            print(f\"Mask Position: {mask_pos}\")\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, mask_pos=torch.tensor([mask_pos]).to(device))\n",
    "            logits = outputs[0]\n",
    "            print(f\"Logits Shape: {logits.shape}\")\n",
    "\n",
    "            if len(logits.shape) == 2 and logits.shape[1] == 2:\n",
    "                roberta_outputs = model.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                hidden_states = roberta_outputs[0]\n",
    "                if hasattr(model, 'lm_head'):\n",
    "                    logits = model.lm_head(hidden_states)\n",
    "                else:\n",
    "                    logits = model.classifier(hidden_states)\n",
    "                    if logits.shape[-1] == 2:\n",
    "                        raise ValueError(\"Model outputs classification logits instead of token logits. Missing LM head?\")\n",
    "                print(f\"Corrected Logits Shape: {logits.shape}\")\n",
    "\n",
    "            logits_at_mask = logits[0, mask_pos]\n",
    "            prob_great = logits_at_mask[label_ids[\"positive\"]].item()\n",
    "            prob_terrible = logits_at_mask[label_ids[\"negative\"]].item()\n",
    "            pred_label = \"positive\" if prob_great > prob_terrible else \"negative\"\n",
    "\n",
    "            print(f\"Predicted: {pred_label}, Expected: {sample['label']}\")\n",
    "            print(f\"Logits - great: {prob_great}, terrible: {prob_terrible}\")\n",
    "            correct += (pred_label == sample[\"label\"])\n",
    "\n",
    "    accuracy = correct / len(test_samples) * 100\n",
    "    print(f\"Evaluation accuracy for {model_dir}: {accuracy}%\")\n",
    "    return accuracy\n",
    "\n",
    "def extract_metrics_from_log(log_file):\n",
    "    eval_loss = None\n",
    "    eval_acc = None\n",
    "    timestamp = None\n",
    "    \n",
    "    with open(log_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if \"eval_loss =\" in line:\n",
    "                timestamp = line.split(\" - \")[0]\n",
    "                eval_loss = float(line.split(\"eval_loss =\")[1].strip())\n",
    "            elif \"eval_acc =\" in line:\n",
    "                eval_acc = float(line.split(\"eval_acc =\")[1].strip())\n",
    "    \n",
    "    return timestamp, eval_loss, eval_acc\n",
    "\n",
    "def save_client_metrics(client_id, round_num, log_file):\n",
    "    timestamp, eval_loss, eval_acc = extract_metrics_from_log(log_file)\n",
    "    if eval_loss is not None and eval_acc is not None and timestamp is not None:\n",
    "        filename = f\"client_{client_id}.txt\"\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(f\"{timestamp} - INFO - __main__ -     eval_loss = {eval_loss}\\n\")\n",
    "            f.write(f\"{timestamp} - INFO - __main__ -     eval_acc = {eval_acc}\\n\")\n",
    "\n",
    "def main():\n",
    "    num_rounds = 6\n",
    "    clients = 4\n",
    "    parent_folder = \"output_model\"\n",
    "    global_model_output_dir = \"saved_model\"\n",
    "\n",
    "    # Clear existing client files\n",
    "    for client_id in range(1, clients + 1):\n",
    "        filename = f\"client_{client_id}.txt\"\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "    for round_num in range(1, num_rounds + 1):\n",
    "        print(f\"Starting Round {round_num}\")\n",
    "        averaged_state_dict = {}\n",
    "\n",
    "        for client_id in range(1, clients + 1):\n",
    "            print(f\"------------------------------------Executing command for Client {client_id} in Round {round_num}:\")\n",
    "            directory = f\"data/k-shot-1k-test/SST-2/512-{client_id}/\"\n",
    "            cached_files = glob.glob(os.path.join(directory, \"cached*\"))\n",
    "            for file_path in cached_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "            command = f\"mv data/k-shot-1k-test/SST-2/512-{client_id}/train_part1.tsv data/k-shot-1k-test/SST-2/512-{client_id}/train.tsv\"\n",
    "            os.system(command)\n",
    "\n",
    "            log_file = f\"mezo_round_{round_num}_client_{client_id}.log\"\n",
    "            if round_num == 1:\n",
    "                command = f\"WANDB_MODE=offline TASK=SST-2 K=512 SEED={client_id} BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL=roberta-large bash mezo.sh\"\n",
    "                os.system(command)\n",
    "            else:\n",
    "                command = f\"WANDB_MODE=offline TASK=SST-2 K=512 SEED={client_id} BS=64 LR=1e-6 STEP=10 EVAL_STEP=10 MODEL={global_model_output_dir} bash mezo.sh\"\n",
    "                os.system(command)\n",
    "            with open(log_file, \"w\") as f:\n",
    "                process = subprocess.run(command, shell=True, stdout=f, stderr=subprocess.STDOUT, text=True)\n",
    "            with open(log_file, \"r\") as f:\n",
    "                print(f\"MeZO stdout:\\n{f.read()}\")\n",
    "            if process.returncode != 0:\n",
    "                raise RuntimeError(\"MeZO execution failed\")\n",
    "\n",
    "            # Save metrics from log\n",
    "            save_client_metrics(client_id, round_num, log_file)\n",
    "\n",
    "            command = f\"mv data/k-shot-1k-test/SST-2/512-{client_id}/train.tsv data/k-shot-1k-test/SST-2/512-{client_id}/train_part1.tsv\"\n",
    "            os.system(command)\n",
    "\n",
    "            model = RobertaModelForPromptFinetuning.from_pretrained(parent_folder)\n",
    "            model.model_args = DynamicTrainingArguments()\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(parent_folder)\n",
    "            state_dict = model.state_dict()\n",
    "\n",
    "            if not averaged_state_dict:\n",
    "                averaged_state_dict = {k: v.clone() for k, v in state_dict.items()}\n",
    "            else:\n",
    "                for key in state_dict.keys():\n",
    "                    if state_dict[key].is_floating_point():\n",
    "                        averaged_state_dict[key] += state_dict[key]\n",
    "                    else:\n",
    "                        averaged_state_dict[key] = state_dict[key].clone()\n",
    "\n",
    "            os.system(f\"rm -rf {parent_folder}\")\n",
    "\n",
    "        print(\"Averaging weights\")\n",
    "        for key in averaged_state_dict.keys():\n",
    "            if averaged_state_dict[key].is_floating_point():\n",
    "                averaged_state_dict[key] /= clients\n",
    "\n",
    "        if round_num == 1:\n",
    "            global_model = RobertaModelForPromptFinetuning.from_pretrained('roberta-large')\n",
    "        else:\n",
    "            global_model = RobertaModelForPromptFinetuning.from_pretrained(global_model_output_dir)\n",
    "        global_model.model_args = DynamicTrainingArguments()\n",
    "        global_model.load_state_dict(averaged_state_dict)\n",
    "        global_model.save_pretrained(global_model_output_dir)\n",
    "        tokenizer.save_pretrained(global_model_output_dir)\n",
    "        print(f\"Global model saved to {global_model_output_dir}\")\n",
    "\n",
    "        evaluate_model(global_model_output_dir)\n",
    "\n",
    "    print(\"All rounds completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef190cc6-fd52-4fc0-8d3a-463297def1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javad/MeZO/medium_models\n"
     ]
    }
   ],
   "source": [
    "%cd MeZO/medium_models/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
